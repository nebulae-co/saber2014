@article{Abraham,
abstract = {Data in many different fields come to practitioners through a process naturally described as functional. Although data are gathered as finite vector and may contain measurement errors, the functional form have to be taken into account. We propose a clustering procedure of such data emphasizing the functional nature of the objects. The new clustering method consists of two stages: fitting the functional data by B-splines and partitioning the estimated model coefficients using a k-means algorithm. Strong consistency of the clustering method is proved and a real-world example from food industry is given.},
author = {Abraham, C and Cornillon, P A and {Matzner L{\o}ber}, E and Molinari, N},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abraham et al. - 2003 - Unsupervised Curve Clustering using B Splines.pdf:pdf},
isbn = {1467-9469},
journal = {Scandinavian Journal of Statistics},
keywords = {b-splines,clustering,epi-convergence,functional data,k -means,partitioning},
number = {3},
pages = {581--595},
title = {{Unsupervised Curve Clustering using B Splines}},
volume = {30},
year = {2003}
}
@article{ABRAMSON,
author = {Abramson, I S},
journal = {The Annals of Statistics},
pages = {1217--1223},
title = {{On Bandwidth Variation in Kernel Estimates-A Square Root Law}},
volume = {10},
year = {1982}
}
@phdthesis{Acosta2015,
author = {Acosta, Juan Pablo},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Acosta - 2015 - Strategy for Multivariate Identification of Diferentially Expressed Genes in Microarray Data.pdf:pdf},
title = {{Strategy for Multivariate Identification of Diferentially Expressed Genes in Microarray Data}},
year = {2015}
}
@misc{Acosta2015,
address = {Bogota, Colombia},
annote = {R package version 1.0.0},
author = {Acosta, Juan Pablo and Lopez-Kleine, Liliana},
title = {{{\{}acde{\}}: Artificial Components Detection of Differentially Expressed Genes}},
year = {2015}
}
@misc{Adomavicius2005,
abstract = { This paper presents an overview of the field of recommender systems and describes the current generation of recommendation methods that are usually classified into the following three main categories: content-based, collaborative, and hybrid recommendation approaches. This paper also describes various limitations of current recommendation methods and discusses possible extensions that can improve recommendation capabilities and make recommender systems applicable to an even broader range of applications. These extensions include, among others, an improvement of understanding of users and items, incorporation of the contextual information into the recommendation process, support for multicriteria ratings, and a provision of more flexible and less intrusive types of recommendations.},
author = {Adomavicius, Gediminas and Tuzhilin, Alexander},
booktitle = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Collaborative filtering,Extensions to recommander systems,Rating estimation methods,Recommander systems},
number = {6},
pages = {734--749},
title = {{Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions}},
volume = {17},
year = {2005}
}
@phdthesis{ALAYON,
author = {Alay{\'{o}}n, R A},
school = {Universidad Distrital Francisco Jos{\'{e}} de Caldas},
title = {{El controlador l{\'{o}}gico difuso}},
year = {2013}
}
@article{Alon2009,
abstract = {Choosing good problems is essential for being a good scientist. But what is a good problem, and how do you choose one? The subject is not usually discussed explicitly within our profession. Scientists are expected to be smart enough to figure it out on their own and through the observation of their teachers. This lack of explicit discussion leaves a vacuum that can lead to approaches such as choosing problems that can give results that merit publication in valued journals, resulting in a job and tenure. ?? 2009 Elsevier Inc. All rights reserved.},
author = {Alon, Uri},
journal = {Molecular Cell},
number = {6},
pages = {726--728},
title = {{How To Choose a Good Scientific Problem}},
volume = {35},
year = {2009}
}
@phdthesis{Aponte2012,
author = {Aponte, Leonardo},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Aponte - 2012 - Propuesta de una prueba basada en permutaciones para la igualdad de K medias bajo heteroscedasticidad.pdf:pdf},
number = {c},
title = {{Propuesta de una prueba basada en permutaciones para la igualdad de K medias bajo heteroscedasticidad}},
year = {2012}
}
@phdthesis{Aponte2012,
author = {Aponte, Leonardo},
number = {c},
title = {{Propuesta de una prueba basada en permutaciones para la igualdad de K medias bajo heteroscedasticidad}},
year = {2012}
}
@article{Applegate2011,
abstract = {Multidimensional distributions are often used in data min- ing to describe and summarize di erent features of large datasets. It is natural to look for distinct classes in such datasets by clustering the data. A common approach entails the use of methods like k-means clustering. However, the k-means method inherently relies on the Euclidean metric in the embedded space and does not account for additional topology underlying the distribution. In this paper, we propose using Earth Mover Distance (EMD) to compare multidimensional distributions. For a n-bin histogram, the EMD is based on a solution to the transportation problem with time complexity O(n3 log n). To mitigate the high computational cost of EMD, we pro- pose an approximation that reduces the cost to linear time. Given the large size of our dataset a fast approximation is crucial for this application. Other notions of distances such as the information theo- retic Kullback-Leibler divergence and statistical 2 distance, account only for the correspondence between bins with the same index, and do not use information across bins, and are sensitive to bin size. A cross-bin distance measure like EMD is not a ected by binning di erences and meaningfully matches the perceptual notion of $\backslash$nearness". Our technique is simple, e cient and practical for clus- tering distributions. We demonstrate the use of EMD on a real-world application of analyzing 411,550 anonymous mo- bility usage patterns which are dened as distributions over a manifold. EMD allows us to represent inherent relation- ships in this space, and enables us to successfully cluster even sparse signatures.},
author = {Applegate, D and Dasu, T and Krishnan, S and Urbanek, S},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Applegate et al. - 2011 - Unsupervised Clustering of Multidimensional Distributions using Earth Mover Distance.pdf:pdf},
isbn = {9781450308137},
journal = {Kdd'11},
keywords = {Algorithms,Similarity measures},
pages = {636--644},
title = {{Unsupervised Clustering of Multidimensional Distributions using Earth Mover Distance}},
year = {2011}
}
@article{Applegate2011,
abstract = {Multidimensional distributions are often used in data min- ing to describe and summarize di erent features of large datasets. It is natural to look for distinct classes in such datasets by clustering the data. A common approach entails the use of methods like k-means clustering. However, the k-means method inherently relies on the Euclidean metric in the embedded space and does not account for additional topology underlying the distribution. In this paper, we propose using Earth Mover Distance (EMD) to compare multidimensional distributions. For a n-bin histogram, the EMD is based on a solution to the transportation problem with time complexity O(n3 log n). To mitigate the high computational cost of EMD, we pro- pose an approximation that reduces the cost to linear time. Given the large size of our dataset a fast approximation is crucial for this application. Other notions of distances such as the information theo- retic Kullback-Leibler divergence and statistical 2 distance, account only for the correspondence between bins with the same index, and do not use information across bins, and are sensitive to bin size. A cross-bin distance measure like EMD is not a ected by binning di erences and meaningfully matches the perceptual notion of $\backslash$nearness". Our technique is simple, e cient and practical for clus- tering distributions. We demonstrate the use of EMD on a real-world application of analyzing 411,550 anonymous mo- bility usage patterns which are dened as distributions over a manifold. EMD allows us to represent inherent relation- ships in this space, and enables us to successfully cluster even sparse signatures.},
author = {Applegate, D and Dasu, T and Krishnan, S and Urbanek, S},
isbn = {9781450308137},
journal = {Kdd'11},
keywords = {Algorithms,Similarity measures},
pages = {636--644},
title = {{Unsupervised Clustering of Multidimensional Distributions using Earth Mover Distance}},
year = {2011}
}
@article{Armbrust2009,
abstract = {Cloud Computing, the long-held dream of computing as a utility, has the potential to transform a large part of the  IT industry, making software even more attractive as a service and shaping the way IT hardware is designed and  purchased. Developers with innovative ideas for new Internet services no longer require the large capital outlays  in hardware to deploy their service or the human expense to operate it. They need not be concerned about over-  provisioning for a service whose popularity does not meet their predictions, thus wasting costly resources, or under-  provisioning for one that becomes wildly popular, thus missing potential customers and revenue. Moreover, companies  with large batch-oriented tasks can get results as quickly as their programs can scale, since using 1000 servers for one  hour costs no more than using one server for 1000 hlarge scale, is unprecedented in the history of IT.  },
author = {Armbrust, M and Fox, A and Griffith, R and Joseph, AD and RH},
journal = {  University of California, Berkeley, Tech. Rep. UCB },
keywords = {cloud computing,distributed system economics,internet datacenters,utility computing},
pages = {07--013},
title = {{Above the clouds: A Berkeley view of cloud computing}},
url = {http://scholar.google.com/scholar?q=intitle:Above+the+clouds:+A+Berkeley+view+of+cloud+computing{\#}0},
year = {2009}
}
@phdthesis{AG36a,
author = {Aspirot, L},
school = {Universidad de la Rep{\{}{\'{u}}{\}}blica. Montevideo},
title = {{Regresi{\'{o}}n no param{\'{e}}trica para datos funcionales no estacionarios}},
year = {2008}
}
@phdthesis{AG36a,
author = {Aspirot, L},
school = {Universidad de la Rep{\'{u}}blica. Montevideo},
title = {{Regresi{\'{o}}n no param{\'{e}}trica para datos funcionales no estacionarios}},
year = {2008}
}
@article{Bacardit2014,
abstract = {Data mining and knowledge discovery techniques have greatly progressed in the last decade. They are now able to handle larger and larger datasets, process heterogeneous information, integrate complex metadata, and extract and visualize new knowledge. Often these advances were driven by new challenges arising from real-world domains, with biology and biotechnology a prime source of diverse and hard (e.g., high volume, high throughput, high variety, and high noise) data analytics problems. The aim of this article is to show the broad spectrum of data mining tasks and challenges present in biological data, and how these challenges have driven us over the years to design new data mining and knowledge discovery procedures for biodata. This is illustrated with the help of two kinds of case studies. The first kind is focused on the field of protein structure prediction, where we have contributed in several areas: by designing, through regression, functions that can distinguish between good and bad models of a protein's predicted structure; by creating new measures to characterize aspects of a protein's structure associated with individual positions in a protein's sequence, measures containing information that might be useful for protein structure prediction; and by creating accurate estimators of these structural aspects. The second kind of case study is focused on omics data analytics, a class of biological data characterized for having extremely high dimensionalities. Our methods were able not only to generate very accurate classification models, but also to discover new biological knowledge that was later ratified by experimentalists. Finally, we describe several strategies to tightly integrate knowledge extraction and data mining in order to create a new class of biodata mining algorithms that can natively embrace the complexity of biological data, efficiently generate accurate information in the form of classification/regression models, and extract valuable new knowledge. Thus, a complete data-to-information-to-knowledge pipeline is presented.},
author = {Bacardit, Jaume and Widera, Pawe{\l} and Lazzarini, Nicola and Krasnogor, Natalio},
doi = {10.1089/big.2014.0023},
issn = {2167-6461},
journal = {Big data},
number = {3},
pages = {164--176},
pmid = {25276500},
title = {{Hard Data Analytics Problems Make for Better Data Analysis Algorithms: Bioinformatics as an Example.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4174911{\{}{\&}{\}}tool=pmcentrez{\{}{\&}{\}}rendertype=abstract},
volume = {2},
year = {2014}
}
@article{SPATSTAT,
author = {Baddeley, Adrian and Turner, Rolf},
journal = {Journal of Statistical Software},
number = {6},
pages = {1--42},
title = {{{\{}spatstat{\}}: An {\{}R{\}} Package for Analyzing Spatial Point Patterns}},
url = {http://www.jstatsoft.org/v12/i06/},
volume = {12},
year = {2005}
}
@article{Bajcsy2005,
abstract = {Recent progress in biology, medical science, bioinformatics, and biotechnology has led to the accumulation of tremendous amounts of biodata that demands in-depth analysis. On the other hand, recent progress in data mining research has led to the development of numerous efficient and scalable methods for mining interesting patterns in large databases. The question becomes how to bridge the two fields, data mining and bioinformatics, for successful mining of biological data. In this chapter, we present an overview of the data mining methods that help biodata analysis. Moreover, we outline some research problems that may motivate the further development of data mining tools for the analysis of various kinds of biological data.},
author = {Bajcsy, Peter and Han, Jiawei and Liu, Lei and Yang, Jiong},
doi = {10.1007/1-84628-059-1{_}2},
isbn = {1852336714},
journal = {Data Mining in Bioinformatics},
pages = {9--39},
title = {{Survey of Biodata Analysis from a Data Mining Perspective}},
url = {http://dx.doi.org/10.1007/1-84628-059-1{\{}{\_}{\}}2},
year = {2005}
}
@article{BANDI,
abstract = {We propose a functional estimation procedure for homogeneous stochastic differential equations based on a discrete sample of observations and with minimal requirements on the data generating process. We show how to identify the drift and diffusion function in situations where one or the other function is considered a nuisance parameter. The asymptotic behavior of the estimators is examined as the observation frequency increases and as the time span lengthens. We prove almost sure consistency and weak convergence to mixtures of normal laws, where the mixing variates depend on the chronological local time of the underlying diffusion process, that is the random time spent by the process in the vicinity of a generic spatial point. The estimation method and asymptotic results apply to both stationary and nonstationary recurrent processes.},
author = {Bandi, F.M. M and Phillips, P.C.B.},
doi = {10.1111/1468-0262.00395},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bandi, Phillips - 2003 - Fully nonparametric estimation of scalar diffusion models.pdf:pdf},
issn = {0012-9682},
journal = {Econometrica},
pages = {241--283},
title = {{Fully Nonparametric Estimation of Scalar Diffusion Models}},
url = {http://dx.doi.org/10.1111/1468-0262.00395},
volume = {71},
year = {2003}
}
@article{Bapat2000,
author = {Bapat, R B},
title = {{Linear Algebra and Linear Models, Second Edition}},
year = {2000}
}
@article{Bapat2000,
author = {Bapat, R B},
title = {{Linear Algebra and Linear Models, Second Edition}},
year = {2000}
}
@misc{Barrera2014,
author = {Barrera, Carlos and Correa, Juan},
booktitle = {Simposio Internacional de Estad{\{}{\'{i}}{\}}stica},
title = {{A proposal of clustering for functional data}},
url = {http://simposioestadistica.unal.edu.co/fileadmin/content/eventos/simposioestadistica/documentos/memorias/Memorias{\{}{\_}{\}}2014.rar},
year = {2014}
}
@misc{Barrera2014,
author = {Barrera, Carlos and Correa, Juan},
booktitle = {Simposio Internacional de Estad{\'{i}}stica},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Barrera, Correa - 2014 - A proposal of clustering for functional data.pdf:pdf},
title = {{A proposal of clustering for functional data}},
url = {http://simposioestadistica.unal.edu.co/fileadmin/content/eventos/simposioestadistica/documentos/memorias/Memorias{\_}2014.rar},
year = {2014}
}
@book{BARTLE,
address = {USA},
author = {Bartle, R},
publisher = {John Wiley {\&} Sons},
title = {{Introduction to the Measure Theory}},
year = {1966}
}
@misc{Berners-Lee2001,
abstract = {A new form of Web content that is meaningful to computers will unleash a revolution of new possibilities.},
author = {Berners-Lee, Tim and Hendler, James and Lassila, Ora},
booktitle = {Scientific American},
number = {5},
pages = {34--43},
title = {{The Semantic Web}},
volume = {284},
year = {2001}
}
@misc{Berners-Lee2001,
abstract = {A new form of Web content that is meaningful to computers will unleash a revolution of new possibilities.},
author = {Berners-Lee, Tim and Hendler, James and Lassila, Ora},
booktitle = {Scientific American},
number = {5},
pages = {34--43},
title = {{The Semantic Web}},
volume = {284},
year = {2001}
}
@article{Biesecker2014,
abstract = {Sequencing of the genome or exome for clinical applications, hereafter referred to as clinical genome and exome sequencing (CGES), has now entered medical practice.1 Several thousand CGES tests have already been ordered for patients, with the goal of establishing diagnoses for rare, clinically unrecognizable, or puzzling disorders that are suspected to be genetic in origin. We anticipate increases in the use of CGES, the key attribute of which — its breadth — distinguishes it from other forms of laboratory testing. The interrogation of variation in about 20,000 genes simultaneously can be a powerful and effective diagnostic method.2 CGES has been hailed as an important tool in the implementation of predictive and individualized medicine, and there is intense research interest in the clinical benefits and risks of sequencing for screening healthy persons3; however, current practice recommendations4 do not support the use of sequencing for this purpose, and for that reason we do not further address it here. We have also limited this overview of CGES to the analysis of germline sequence variants for diagnostic purposes and do not discuss the use of CGES to uncover somatic variants in cancer in order to individualize cancer therapy. Clinicians should understand the diagnostic indications for CGES so that they can effectively deploy it in their practices. Because the success rate of CGES for the identification of a causative variant is approximately 25{\{}{\%}{\}},5 it is important to understand the basis of this testing and how to select the patients most likely to benefit from it. Here, we summarize the technologies underlying CGES and offer our insights into how clinicians should order such testing, interpret the results, and communicate the results to their patients (an interactive graphic giving an overview of the process is available with the full text of this article at NEJM.org).},
author = {Biesecker, Leslie G and Green, Robert C},
doi = {10.1056/NEJMra1312543},
isbn = {1533-4406 (Electronic)$\backslash$r0028-4793 (Linking)},
issn = {1533-4406},
journal = {New England Journal of Medicine},
keywords = {DNA,DNA: methods,Exome,Genetic Counseling,Genetic Diseases,Genetic Testing,Genome,Human,Humans,Inborn,Inborn: diagnosis,Inborn: genetics,Phenotype,Sequence Analysis},
number = {25},
pages = {2418--2425},
pmid = {24941179},
title = {{Diagnostic Clinical Genome and Exome Sequencing}},
url = {http://www.nejm.org/doi/abs/10.1056/NEJMra1312543},
volume = {370},
year = {2014}
}
@book{Bishop2006a,
abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. A forthcoming companion volume will deal with practical aspects of pattern recognition and machine learning, and will include free software implementations of the key algorithms along with example data sets and demonstration programs. Christopher Bishop is Assistant Director at Microsoft Research Cambridge, and also holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, and was recently elected Fellow of the Royal Academy of Engineering. The author's previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Bishop, Christopher M CM Christopher M.},
booktitle = {Pattern Recognition},
chapter = {Graphical},
doi = {10.1117/1.2819119},
editor = {Jordan, M and Kleinberg, J and Sch{\"{o}}lkopf, B},
eprint = {0-387-31073-8},
isbn = {978-0387310732},
issn = {10179909},
number = {4},
pages = {738},
pmid = {8943268},
publisher = {Springer},
series = {Information science and statistics},
title = {{Pattern Recognition and Machine Learning}},
url = {http://www.library.wisc.edu/selectedtocs/bg0137.pdf http://soic.iupui.edu/syllabi/semesters/4142/INFO{\_}B529{\_}Liu{\_}s.pdf$\backslash$nhttp://www.library.wisc.edu/selectedtocs/bg0137.pdf},
volume = {4},
year = {2006}
}
@article{Blei2012,
abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Blei, David M and Ng, Andrew Y and Jordan, Michael I},
chapter = {993},
doi = {10.1162/jmlr.2003.3.4-5.993},
editor = {Lafferty, John},
eprint = {1111.6189v1},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Blei, Ng, Jordan - 2012 - Latent Dirichlet Allocation.pdf:pdf},
institution = {University of California},
isbn = {9781577352815},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {lda,topic model},
number = {4-5},
pages = {993--1022},
pmid = {21362469},
publisher = {JMLR. org},
title = {{Latent Dirichlet Allocation}},
url = {http://www.cs.princeton.edu/{~}blei/lda-c/$\backslash$npapers2://publication/doi/10.1162/jmlr.2003.3.4-5.993$\backslash$npapers2://publication/uuid/4001D0D9-4F9C-4D8F-AE49-46ED6A224F4A$\backslash$npapers2://publication/uuid/7D10D5DA-B421-4D94-A3ED-028107B7F9B6$\backslash$nhttp://www.crossref.org/jmlr},
volume = {3},
year = {2012}
}
@article{BOWMAN,
author = {Bowman, A W},
journal = {Biometrika},
pages = {353--360},
title = {{An alternative method of cross-validation for the smoothing of density estimates}},
volume = {71(2)},
year = {1984}
}
@book{Boyd2004,
abstract = {We are developing a dual panel breast-dedicated PET system using LSO scintillators coupled to position sensitive avalanche photodiodes (PSAPD). The charge output is amplified and read using NOVA RENA-3 ASICs. This paper shows that the coincidence timing resolution of the RENA-3 ASIC can be improved using certain list-mode calibrations. We treat the calibration problem as a convex optimization problem and use the RENA-3s analog-based timing system to correct the measured data for time dispersion effects from correlated noise, PSAPD signal delays and varying signal amplitudes. The direct solution to the optimization problem involves a matrix inversion that grows order (n3) with the number of parameters. An iterative method using single-coordinate descent to approximate the inversion grows order (n). The inversion does not need to run to convergence, since any gains at high iteration number will be low compared to noise amplification. The system calibration method is demonstrated with measured pulser data as well as with two LSO-PSAPD detectors in electronic coincidence. After applying the algorithm, the 511keV photopeak paired coincidence time resolution from the LSO-PSAPD detectors under study improved by 57{\%}, from the raw value of 16.30.07 ns FWHM to 6.920.02 ns FWHM (11.520.05 ns to 4.890.02 ns for unpaired photons).},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Boyd, Stephen SP and Vandenberghe, Lieven},
booktitle = {Optimization Methods and Software},
chapter = {1,10,11},
doi = {10.1080/10556781003625177},
eprint = {1111.6189v1},
isbn = {9780521833783},
issn = {10556788},
number = {3},
pages = {487--487},
pmid = {20876008},
publisher = {Cambridge University Press},
title = {{Convex optimization}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=mYm0bLd3fcoC{\&}oi=fnd{\&}pg=PR11{\&}dq=Convex+Optimization{\&}ots=tc6WzCKHP3{\&}sig=5azqvdi0EX0id0Boq{\_}bYNJmB68E$\backslash$nhttp://www.informaworld.com/openurl?genre=article{\&}doi=10.1080/10556781003625177{\&}magic=crossref},
volume = {25},
year = {2004}
}
@article{Brin2012,
abstract = {In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The prototype with a full text and hyperlink database of at least 24 million pages is available at http://google.stanford.edu/ To engineer a search engine is a challenging task. Search engines index tens to hundreds of millions of web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and web proliferation, creating a web search engine today is very different from 3 years ago. This paper provides an in-depth description of our large-scale web search engine - the first such detailed public description we know of to date. Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical large-scale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections, where anyone can publish anything they want. ?? 2012 Elsevier B.V. All rights reserved.},
author = {Brin, Sergey and Page, Lawrence},
journal = {Computer Networks},
keywords = {Google,Information retrieval,Pagerank,Search engines,World Wide Web},
number = {18},
pages = {3825--3833},
title = {{Reprint of: The anatomy of a large-scale hypertextual web search engine}},
volume = {56},
year = {2012}
}
@article{Burges1998,
abstract = {The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, working through a non-trivial example in detail. We describe a mechanical analogy, and discuss when SVM solutions are unique and when they are global. We describe how support vector training can be practically implemented, and discuss in detail the kernel mapping technique which is used to construct SVM solutions which are nonlinear in the data. We show how Support Vector machines can have very large (even infinite) VC dimension by computing the VC dimension for homogeneous polynomial and Gaussian radial basis function kernels. While very high VC dimension would normally bode ill for generalization performance, and while at present there exists no theory which shows that good generalization performance is guaranteed for SVMs, there are several arguments which support the observed high accuracy of SVMs, which we review. Results of some experiments which were inspired by these arguments are also presented. We give numerous examples and proofs of most of the key theorems. There is new material, and I hope that the reader will find that even old material is cast in a fresh light.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Burges, CJC Christopher J C},
doi = {10.1023/A:1009715923555},
editor = {Fayyad, Usama},
eprint = {1111.6189v1},
institution = {Bell Laboratories, Lucent Technologies},
isbn = {0818672404},
issn = {13845810},
journal = {Data Mining and Knowledge Discovery},
keywords = {pattern recognition,statistical learning theory,support vector machines,vc dimension},
number = {2},
pages = {121--167},
pmid = {5207842081938259593},
publisher = {Springer},
series = {NetGames '06},
title = {{A Tutorial on Support Vector Machines for Pattern Recognition}},
url = {http://www.springerlink.com/index/Q87856173126771Q.pdf$\backslash$nhttp://link.springer.com/article/10.1023/A:1009715923555},
volume = {2},
year = {1998}
}
@article{Canuel2015,
author = {Canuel, V and Rance, B and Avillach, P and Degoulet, P and Burgun, A},
doi = {10.1093/bib/bbu006},
issn = {1467-5463},
journal = {Briefings in Bioinformatics},
keywords = {biomedical research,clinical data,high-throughput technologies,information storage,translational medical research},
number = {2},
pages = {280--290},
title = {{Translational research platforms integrating clinical and omics data: a review of publicly available solutions}},
url = {http://bib.oxfordjournals.org/cgi/doi/10.1093/bib/bbu006},
volume = {16},
year = {2015}
}
@article{CARSWA,
abstract = {Computer simulation techniques were used to study the Type I and Type Ill error rates and the correct decision rates for ten pairwise multiple comparison procedures. Results indicated that Scheffh's test, Tukey's test, and the Student-Newman-Keuls test are less ap- propriate than either the least significant difference with the restric- tion that the analysis of variance F value be significant at or = .05, two Bayesian modifications of the least significant difference, or Duncan's multiple range test. Because of its ease of application, many researchers may prefer the restricted least significant difference. 1.},
author = {Carmer, S G and Swanson, M R},
doi = {10.2307/2284140},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cramer, Swanson - 1973 - An Evaluation of Ten Pairwise Multiple Comparison Procedures by Monte Carlo Methos.pdf:pdf},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Monte Carlo Method,Statistics},
number = {341},
pages = {66--74},
title = {{An evaluation of ten pairwise multiple comparison procedures by Monte Carlo methods}},
volume = {68},
year = {1973}
}
@article{CARSWA,
abstract = {Computer simulation techniques were used to study the Type I and Type Ill error rates and the correct decision rates for ten pairwise multiple comparison procedures. Results indicated that Scheffh's test, Tukey's test, and the Student-Newman-Keuls test are less ap- propriate than either the least significant difference with the restric- tion that the analysis of variance F value be significant at or = .05, two Bayesian modifications of the least significant difference, or Duncan's multiple range test. Because of its ease of application, many researchers may prefer the restricted least significant difference. 1.},
author = {Carmer, S G and Swanson, M R},
doi = {10.2307/2284140},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Monte Carlo Method,Statistics},
number = {341},
pages = {66--74},
title = {{An evaluation of ten pairwise multiple comparison procedures by Monte Carlo methods}},
volume = {68},
year = {1973}
}
@inproceedings{Chang2006,
abstract = {Bigtable is a distributed storage system for managing structured data that is designed to scale to a very large size: petabytes of data across thousands of commodity servers. Many projects at Google store data in Bigtable, including web indexing, Google Earth, and Google Fi- nance. These applications place very different demands on Bigtable, both in terms of data size (from URLs to web pages to satellite imagery) and latency requirements (from backend bulk processing to real-time data serving). Despite these varied demands, Bigtable has successfully provided a flexible, high-performance solution for all of these Google products. In this paper we describe the sim- ple data model provided by Bigtable, which gives clients dynamic control over data layout and format, and we de- scribe the design and implementation of Bigtable.},
author = {Chang, Fay and Dean, Jeffrey and Ghemawat, Sanjay and Hsieh, Wilson C. and Wallach, Deborah A. and Burrows, Mike and Chandra, Tushar and Fikes, Andrew and Gruber, Robert E.},
booktitle = {7th Symposium on Operating Systems Design and Implementation (OSDI '06), November 6-8, Seattle, WA, USA},
pages = {205--218},
publisher = {USENIX Association},
title = {{Bigtable: A distributed storage system for structured data}},
url = {http://research.google.com/archive/bigtable-osdi06.pdf},
year = {2006}
}
@article{Cheng2015,
author = {Cheng, Phil and Levesque, Mitch and Cheng, Phil F and Dummer, Reinhard and Levesque, Mitch P},
doi = {10.4414/smw.2015.14183},
keywords = {Cancer Gen,Genomics,cancer are somatic mutation,copy number,data mining,gene expres-,genomics,on the tcga data,portal for each,the cancer genome atlas,the data types listed,transcriptomics},
number = {October},
pages = {1--5},
title = {{Data mining The Cancer Genome Atlas in the era of precision cancer medicine Data mining The Cancer Genome Atlas in the era of precision cancer medicine}},
year = {2015}
}
@book{LEBESGUE,
address = {Madison, WI 53706-1685},
author = {Cheng, S},
publisher = {Springer},
title = {{A Crash Course on the Lebesgue Integral and Measure Theory}},
year = {2008}
}
@article{Chih-WeiHsuChih-ChungChang2008,
abstract = {The support vector machine (SVM) is a popular classi cation technique. However, beginners who are not familiar with SVM often get unsatisfactory results since they miss some easy but signi cant steps. In this guide, we propose a simple procedure which usually gives reasonable results. developed well-differentiated superficial transitional cell bladder cancer. CONCLUSIONS: Patients with SCI often prefer SPC than other methods offered to them, because of quality-of-life issues. The incidence of significant complications might not be as high as previously reported, and with a commitment to careful follow-up, SPC can be a safe option for carefully selected patients if adequate surveillance can be ensured.},
author = {{Chih-Wei Hsu, Chih-Chung Chang}, and Chih-Jen Lin},
journal = {BJU international},
number = {1},
pages = {1396--400},
title = {{A Practical Guide to Support Vector Classification}},
url = {http://www.csie.ntu.edu.tw/{~}cjlin/papers/guide/guide.pdf},
volume = {101},
year = {2008}
}
@techreport{Chu2011,
abstract = {SAM (Significance Analysis of Microarrays) is a statistical technique for finding significant genes in a set of microarray experiments. It was proposed by Tusher, Tibshirani and Chu [9]. The software was written by Balasubramanian Narasimhan and Robert Tibshirani. The input to SAM is gene expression measurements from a set of microarray experiments, as well as a response variable from each experiment. The response variable may be a grouping like untreated, treated [either unpaired or paired], a multiclass grouping (like breast cancer, lymphoma, colon cancer, . . . ), a quantitative variable (like blood pressure) or a possibly censored survival time. SAM computes a statistic di for each gene i, measuring the strength of the relationship between gene expression and the response variable. It uses repeated permutations of the data to determine if the expression of any genes are significantly related to the response. The cutoff for significance is determined by a tuning parameter delta, chosen by the user based on the false positive rate. One can also choose a fold change parameter, to ensure that called genes change at least a pre-specified amount.},
author = {Chu, Gil and Li, Jun and Narasimhan, Balasubramanian and Tibshirani, Robert and Tusher, Virginia},
booktitle = {Policy},
doi = {10.1261/rna.1473809},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chu et al. - 2011 - SAM - Significance Analysis of Microarrays - Users guide and technical document.pdf:pdf},
issn = {1469-9001},
keywords = {gene expression,microarrays,significance analysi},
pages = {1--42},
pmid = {19307293},
title = {{SAM - Significance Analysis of Microarrays - Users guide and technical document}},
year = {2011}
}
@techreport{Chu2011,
abstract = {SAM (Significance Analysis of Microarrays) is a statistical technique for finding significant genes in a set of microarray experiments. It was proposed by Tusher, Tibshirani and Chu [9]. The software was written by Balasubramanian Narasimhan and Robert Tibshirani. The input to SAM is gene expression measurements from a set of microarray experiments, as well as a response variable from each experiment. The response variable may be a grouping like untreated, treated [either unpaired or paired], a multiclass grouping (like breast cancer, lymphoma, colon cancer, . . . ), a quantitative variable (like blood pressure) or a possibly censored survival time. SAM computes a statistic di for each gene i, measuring the strength of the relationship between gene expression and the response variable. It uses repeated permutations of the data to determine if the expression of any genes are significantly related to the response. The cutoff for significance is determined by a tuning parameter delta, chosen by the user based on the false positive rate. One can also choose a fold change parameter, to ensure that called genes change at least a pre-specified amount.},
author = {Chu, Gil and Li, Jun and Narasimhan, Balasubramanian and Tibshirani, Robert and Tusher, Virginia},
booktitle = {Policy},
doi = {10.1261/rna.1473809},
issn = {1469-9001},
keywords = {gene expression,microarrays,significance analysi},
pages = {1--42},
pmid = {19307293},
title = {{SAM - Significance Analysis of Microarrays - Users guide and technical document}},
year = {2011}
}
@article{CLARKEVANS,
author = {Clark, P and Evans, F},
journal = {Ecology},
pages = {445--453},
title = {{Distance to nearest neighbor as a measure of spatial relationshipts in ppulations}},
volume = {35},
year = {1954}
}
@phdthesis{COR,
author = {Corninstein, R},
school = {Universidad de Buenos Aires},
title = {{Estimadores de Naradaya Watson aplicados a datos funcionales}},
year = {2013}
}
@article{Cossio2012,
abstract = {The objective of this case study was to obtain some first-hand information about the functional consequences of a cosmetic tongue split operation for speech and tongue motility. One male patient who had performed the operation on himself was interviewed and underwent a tongue motility assessment, as well as an ultrasound examination. Tongue motility was mildly reduced as a result of tissue scarring. Speech was rated to be fully intelligible and highly acceptable by 4 raters, although 2 raters noticed slight distortions of the sibilants /s/ and /z/. The 3-dimensional ultrasound demonstrated that the synergy of the 2 sides of the tongue was preserved. A notably deep posterior genioglossus furrow indicated compensation for the reduced length of the tongue blade. It is concluded that the tongue split procedure did not significantly affect the participant's speech intelligibility and tongue motility.},
author = {Cossio, Mar{\'{i}}a Laura T and Giesen, Laura F and Araya, Gabriela and P{\'{e}}rez-Cotapos, Mar{\'{i}}a Luisa S and VERGARA, RICARDO L{\'{O}}PEZ and Manca, Maura and Tohme, R A and Holmberg, S D and Bressmann, Tim and Lirio, Daniel Rodrigues and Rom{\'{a}}n, Jelitza Soto and Sol{\'{i}}s, Rodrigo Ganter and Thakur, Sanjay and Rao, S V D Nageswara and Modelado, E L and La, Artificial D E and Durante, Cabeza and Tradici{\'{o}}n, U N A and En, Maya and Espejo, E L and Fuentes, D E L A S and Yucat{\'{a}}n, Universidad Aut{\'{o}}noma De and Lenin, Cruz Moreno and Cian, Laura Franco and Douglas, M Joanne and Plata, La and H{\'{e}}ritier, Fran{\c{c}}oise},
doi = {10.1007/s13398-014-0173-7.2},
isbn = {9780874216561},
issn = {0717-6163},
journal = {Uma {\{}{\'{e}}{\}}tica para quantos?},
keywords = {Adolescence,Adolescencia,Adolescent,Adolescent Behavior,Adolescent Behavior: psychology,Adult,Agresiones al cuerpo,Attachment to the body,Attaque au corps,Autolesiones deliberadas,Automutilation d{\{}{\'{e}}{\}}lib{\{}{\'{e}}{\}}r{\{}{\'{e}}{\}}e,Body Piercing,Body Piercing: psychology,Body Piercing: statistics {\{}{\&}{\}} numerical data,Body image,CUERPO,Chile,Chile: epidemiology,Cosmetic Techniques,Deliberate self-harm,Epidemiologic Methods,Female,Humans,Image corporelle,Imagen corporal,JUVENTUD,MODIFICACIONES CORPORALES,Male,Motivation,Movement,Risk-Taking,Self Mutilation,Self Mutilation: physiopathology,Self Mutilation: ultrasonography,Sex Distribution,Speech Articulation Tests,Speech Intelligibility,Tattooing,Tattooing: psychology,Tattooing: statistics {\{}{\&}{\}} numerical data,Tongue,Tongue: injuries,Tongue: physiopathology,Tongue: ultrasonography,aesthetics,and on cor-,as none were found,autoinjury and health,body,complications did not,complications from inserting a,constituci{\{}{\'{o}}{\}}n del yo,control postural- estabilizaci{\{}{\'{o}}{\}}n- v{\{}{\'{i}}{\}}as,corporal modifications,corps,cuerpo,culturas juveniles,cultures juv{\{}{\'{e}}{\}}niles,epidural,esth{\{}{\'{e}}{\}}tique,est{\{}{\'{e}}{\}}tica,find any reports of,high resolution images,if neuraxial anes-,ing with neuraxial anesthesia,jeunesse,juvenile cultures,juventud,mecanismos de anteroalimentaci{\{}{\'{o}}{\}}n y,modificacio -,needle through a,nes corporales,perforaci{\{}{\'{o}}{\}}n corporal,piel,pr{\{}{\'{a}}{\}}ctica autolesiva,psicoan{\{}{\'{a}}{\}}lisis,research,retroalimentaci{\{}{\'{o}}{\}}n,risks management,segunda piel,sensitivas y motoras,spinal,sustainable reconstruction,tattoo,tattooing,tattoos,tatuaje,the literature on tattoos,was reviewed to see,youth},
number = {2},
pages = {81--87},
pmid = {15003161},
title = {{No Title No Title}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15003161 http://cid.oxfordjournals.org/lookup/doi/10.1093/cid/cir991 http://www.scielo.cl/pdf/udecada/v15n26/art06.pdf http://www.scopus.com/inward/record.url?eid=2-s2.0-84861150233{\{}{\&}{\}}partnerID=tZOtx3y1},
volume = {XXXIII},
year = {2012}
}
@article{Cramer1973,
annote = {Evaluaci{\{}{\'{o}}{\}}n de 10 pruebas distintas. Recopilaci{\{}{\'{o}}{\}}n.},
author = {Cramer, S G and Swanson, M R},
journal = {Journal of the American Statistical Association},
number = {341},
pages = {66--74},
title = {{An Evaluation of Ten Pairwise Multiple Comparison Procedures by Monte Carlo Methos}},
volume = {68},
year = {1973}
}
@article{Cramer1973,
annote = {Evaluaci{\'{o}}n de 10 pruebas distintas. Recopilaci{\'{o}}n.},
author = {Cramer, S. G. and Swanson, M. R.},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cramer, Swanson - 1973 - An Evaluation of Ten Pairwise Multiple Comparison Procedures by Monte Carlo Methos.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {341},
pages = {66--74},
title = {{An Evaluation of Ten Pairwise Multiple Comparison Procedures by Monte Carlo Methos}},
volume = {68},
year = {1973}
}
@article{Dean2008a,
abstract = {MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.},
archivePrefix = {arXiv},
arxivId = {10.1.1.163.5292},
author = {Dean, Jeffrey and Ghemawat, Sanjay},
doi = {10.1145/1327452.1327492},
eprint = {10.1.1.163.5292},
institution = {Google, Inc.},
isbn = {9781595936868},
issn = {00010782},
journal = {Communications of the ACM},
number = {1},
pages = {1--13},
pmid = {11687618},
publisher = {ACM},
series = {SIGMOD '07},
title = {{MapReduce : Simplified Data Processing on Large Clusters}},
url = {http://www.usenix.org/events/osdi04/tech/full{\_}papers/dean/dean{\_}html/ http://portal.acm.org/citation.cfm?id=1327492},
volume = {51},
year = {2008}
}
@article{Dean2008a,
abstract = {MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.},
archivePrefix = {arXiv},
arxivId = {10.1.1.163.5292},
author = {Dean, J and Ghemawat, S},
doi = {10.1145/1327452.1327492},
eprint = {10.1.1.163.5292},
institution = {Google, Inc.},
isbn = {9781595936868},
issn = {00010782},
journal = {Communications of the ACM},
number = {1},
pages = {1--13},
pmid = {11687618},
publisher = {ACM},
series = {SIGMOD '07},
title = {{MapReduce : Simplified Data Processing on Large Clusters}},
url = {http://www.usenix.org/events/osdi04/tech/full{\_}papers/dean/dean{\_}html/},
volume = {51},
year = {2008}
}
@book{DELICADO,
address = {Catalunya},
author = {Delicado, P},
publisher = {Universitat Polit{\`{e}}cnica de Catalunya},
title = {{Curso de Modelos no Param{\'{e}}tricos}},
year = {2008}
}
@book{DELICADO,
address = {Catalunya},
author = {Delicado, P},
publisher = {Universitat Polit{\{}{\`{e}}{\}}cnica de Catalunya},
title = {{Curso de Modelos no Param{\{}{\'{e}}{\}}tricos}},
year = {2008}
}
@article{Del2014,
author = {Del, Optimizaci{\'{o}}n and Al, Tiempo Puerta-electrocardiograma and Una, Interior D E and Cr{\'{i}}tica, Ruta and El, E N},
pages = {51--68},
title = {{Salud P{\{}{\'{u}}{\}}blica • Epidemiolog{\{}{\'{i}}{\}}a Public Health • Epidemiology}},
volume = {2},
year = {2014}
}
@misc{Paez2012,
abstract = {El contexto din{\{}{\'{a}}{\}}mico y competitivo de la organizaci{\{}{\'{o}}{\}}n actual exige permanentes soluciones inform{\{}{\'{a}}{\}}ticas que apoyen efectivamente sus estrategias y objetivos. Las bodegas de datos- Datawarehouse, han incursionado en el mercado como una soluci{\{}{\'{o}}{\}}n innovadora al problema del manejo de datos enmarcando dicha soluci{\{}{\'{o}}{\}}n mayormente, desde el punto de vista tecnol{\{}{\'{o}}{\}}gico, sin considerar los aspectos organizacionales y metodol{\{}{\'{o}}{\}}gicos involucrados.},
author = {de P{\'{a}}ez, Raquel Anaya},
booktitle = {Revista Universidad EAFIT},
issn = {0120-341X},
keywords = {Sistemas de informaci{\{}{\'{o}}{\}}n en administraci{\{}{\'{o}}{\}}n,Trabajo intelectual. Universidad EAFIT},
number = {104},
pages = {93--101},
title = {{Las bodegas de datos como apoyo a los sistemas de informaci{\{}{\'{o}}{\}}n acerca del negocio}},
url = {http://publicaciones.eafit.edu.co/index.php/revista-universidad-eafit/article/view/1176},
volume = {32},
year = {2012}
}
@article{DEVROYE,
author = {Devroye, L and Krzyak, A},
journal = {Statistics and Probability Letters},
pages = {299--308},
title = {{On the Hilbert kernel density estimate.}},
volume = {44},
year = {1999}
}
@article{Dudley2010,
abstract = {With the continued exponential expansion of publicly available genomic data and access to low-cost, high-throughput molecular technologies for profiling patient populations, computational technologies and informatics are becoming vital considerations in genomic medicine. Although cloud computing technology is being heralded as a key enabling technology for the future of genomic research, available case studies are limited to applications in the domain of high-throughput sequence data analysis. The goal of this study was to evaluate the computational and economic characteristics of cloud computing in performing a large-scale data integration and analysis representative of research problems in genomic medicine. We find that the cloud-based analysis compares favorably in both performance and cost in comparison to a local computational cluster, suggesting that cloud computing technologies might be a viable resource for facilitating large-scale translational research in genomic medicine.},
author = {Dudley, Joel T and Pouliot, Yannick and Chen, Rong and Morgan, Alexander A and Butte, Atul J},
doi = {10.1186/gm172},
isbn = {1756-994X (Electronic)},
issn = {1756-994X},
journal = {Genome medicine},
number = {8},
pages = {51},
pmid = {20691073},
title = {{Translational bioinformatics in the cloud: an affordable alternative.}},
url = {http://genomemedicine.com/content/2/8/51},
volume = {2},
year = {2010}
}
@article{DUIN,
author = {Duin, R P W},
journal = {IEEE Transactions on Computers},
pages = {1175--1179},
title = {{On the choice of smoothing parameters for parzen estimators of probability density functions}},
volume = {611},
year = {1976}
}
@article{DUNCAN,
author = {Duncan, D},
doi = {10.1002/0471667196.ess0146.pub2},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Duncan - 1955 - Multiple Range and Multiple F Tests.pdf:pdf},
isbn = {9780471667193},
journal = {Biometrics},
number = {1},
pages = {1--42},
title = {{Multiple Range and Multiple F Tests}},
volume = {11},
year = {1955}
}
@article{DUNNET,
author = {Dunnet, C},
journal = {Journal of the American Statistical Association},
pages = {1096--1121},
title = {{A multiple comparison procedure for comparing several tratments with a control}},
volume = {50},
year = {1955}
}
@article{DUNN,
annote = {From Duplicate 2 (Multiple Comparisons Among Means - Dunn, O J)

Prueba de Dunn.},
author = {Dunn, O J},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dunn - 2014 - Multiple Comparisons Among Means.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {450},
pages = {52--64},
title = {{Multiple Comparisons Among Means}},
volume = {56},
year = {1961}
}
@article{Egozcue2006,
abstract = {The set of probability functions is a convex subset of L 1 and it does not have a linear space structure when using ordinary sum and multiplication by real constants. Moreover, difficulties arise when dealing with distances between densities. The crucial point is that usual distances are not invariant under relevant transformations of densities. To overcome these limitations, Aitchison's ideas on compositional data analysis are used, generalizing perturbation and power transformation, as well as the Aitchison inner product, to operations on probability density functions with support on a finite interval. With these operations at hand, it is shown that the set of bounded probability density functions on finite intervals is a pre-Hilbert space. A Hilbert space of densities, whose logarithm is square-integrable, is obtained as the natural completion of the pre-Hilbert space.},
author = {Egozcue, J J and Iaz-Barrero, J L and Pawlowsky-Glahn, V},
doi = {10.1007/s10114-005-0678-2},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Egozcue, Iaz-Barrero, Pawlowsky-Glahn - 2006 - Hilbert Space of Probability Density Functions Based on Aitchison Geometry.pdf:pdf},
journal = {Acta Mathematica Sinica, English Series Jul},
keywords = {42B05,Aitchison distance,Bayes' theorem,Fourier coefficients,Haar basis,Simplex},
number = {4},
pages = {1175--1182},
title = {{Hilbert Space of Probability Density Functions Based on Aitchison Geometry}},
volume = {22},
year = {2006}
}
@article{Fasano1987,
abstract = {Mon. Not. R. astr. Soc. (1987) 225, 155-170 A multidimensional version of the Kolmogorov - Smirnov test G. Fasano OsservatorioAstronomico, Vicolo dell'Osservatorio 5, 1-35122 Padova, Italy A . Franceschini Istituto diAstronomia, Vicolo dell'Osservatorio5, 1-35122 Padova, Italy ...},
archivePrefix = {arXiv},
arxivId = {arXiv:gr-qc/9809069v1},
author = {Fasano, G and Franceschini, a},
doi = {10.1007/s10342-011-0499-z},
eprint = {9809069v1},
isbn = {9780874216561},
issn = {00358711},
journal = {Monthly Notices of the Royal {\ldots}},
pages = {155--170},
pmid = {15003161},
primaryClass = {arXiv:gr-qc},
title = {{A multidimensional version of the Kolmogorov-Smirnov test}},
url = {http://adsabs.harvard.edu/full/1987mnras.225..155f},
volume = {225},
year = {1987}
}
@article{Friedman1937,
annote = {From Duplicate 2 (Use of Ranks to Avoid the Assumption of Normality Implicit in the Analysis of Variance - Friedman, Milton)

Prueba de Friedman},
author = {Friedman, Milton},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Friedman - 1937 - Use of Ranks to Avoid the Assumption of Normality Implicit in the Analysis of Variance.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {200},
pages = {675--701},
title = {{Use of Ranks to Avoid the Assumption of Normality Implicit in the Analysis of Variance}},
volume = {32},
year = {1937}
}
@book{GAR,
address = {Extremadura},
author = {Garc{\'{i}}a, A},
publisher = {Universidad de Extremadura},
title = {{Teor{\'{i}}as de la Medida y de la Probabilidad}},
year = {2008}
}
@book{GAR,
address = {Extremadura},
author = {Garc{\'{i}}a, A},
publisher = {Universidad de Extremadura},
title = {{Teor{\{}{\'{i}}{\}}as de la Medida y de la Probabilidad}},
year = {2008}
}
@article{Garcia2010,
abstract = {Experimental analysis of the performance of a proposed method is a crucial and necessary task in an investigation. In this paper, we focus on the use of nonparametric statistical inference for analyzing the results obtained in an experiment design in the field of computational intelligence. We present a case study which involves a set of techniques in classification tasks and we study a set of nonparametric procedures useful to analyze the behavior of a method with respect to a set of algorithms, such as the framework in which a new proposal is developed. Particularly, we discuss some basic and advanced nonparametric approaches which improve the results offered by the Friedman test in some circumstances. A set of post hoc procedures for multiple comparisons is presented together with the computation of adjusted p-values. We also perform an experimental analysis for comparing their power, with the objective of detecting the advantages and disadvantages of the statistical tests described. We found that some aspects such as the number of algorithms, number of data sets and differences in performance offered by the control method are very influential in the statistical tests studied. Our final goal is to offer a complete guideline for the use of nonparametric statistical procedures for performing multiple comparisons in experimental studies. {\textcopyright} 2009 Elsevier Inc. All rights reserved.},
annote = {Mirar muy bien,},
author = {Garc{\'{i}}a, Salvador and Fern{\'{a}}ndez, Alberto and Luengo, Juli{\'{a}}n and Herrera, Francisco},
doi = {10.1016/j.ins.2009.12.010},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Garc{\'{i}}a et al. - 2010 - Advanced nonparametric tests for multiple comparisons in the design of experiments in computational intelligence.pdf:pdf},
isbn = {0020-0255},
issn = {00200255},
journal = {Information Sciences},
keywords = {Computational intelligence,Data mining,Fuzzy classification systems,Genetics-based machine learning,Multiple comparisons procedures,Nonparametric statistics,Statistical analysis},
number = {10},
pages = {2044--2064},
publisher = {Elsevier Inc.},
title = {{Advanced nonparametric tests for multiple comparisons in the design of experiments in computational intelligence and data mining: Experimental analysis of power}},
url = {http://dx.doi.org/10.1016/j.ins.2009.12.010},
volume = {180},
year = {2010}
}
@article{Garcia2010,
abstract = {Experimental analysis of the performance of a proposed method is a crucial and necessary task in an investigation. In this paper, we focus on the use of nonparametric statistical inference for analyzing the results obtained in an experiment design in the field of computational intelligence. We present a case study which involves a set of techniques in classification tasks and we study a set of nonparametric procedures useful to analyze the behavior of a method with respect to a set of algorithms, such as the framework in which a new proposal is developed. Particularly, we discuss some basic and advanced nonparametric approaches which improve the results offered by the Friedman test in some circumstances. A set of post hoc procedures for multiple comparisons is presented together with the computation of adjusted p-values. We also perform an experimental analysis for comparing their power, with the objective of detecting the advantages and disadvantages of the statistical tests described. We found that some aspects such as the number of algorithms, number of data sets and differences in performance offered by the control method are very influential in the statistical tests studied. Our final goal is to offer a complete guideline for the use of nonparametric statistical procedures for performing multiple comparisons in experimental studies. {\textcopyright} 2009 Elsevier Inc. All rights reserved.},
annote = {Mirar muy bien,},
author = {Garc{\'{i}}a, Salvador and Fern{\'{a}}ndez, Alberto and Luengo, Juli{\'{a}}n and Herrera, Francisco},
doi = {10.1016/j.ins.2009.12.010},
isbn = {0020-0255},
issn = {00200255},
journal = {Information Sciences},
keywords = {Computational intelligence,Data mining,Fuzzy classification systems,Genetics-based machine learning,Multiple comparisons procedures,Nonparametric statistics,Statistical analysis},
number = {10},
pages = {2044--2064},
publisher = {Elsevier Inc.},
title = {{Advanced nonparametric tests for multiple comparisons in the design of experiments in computational intelligence and data mining: Experimental analysis of power}},
url = {http://dx.doi.org/10.1016/j.ins.2009.12.010},
volume = {180},
year = {2010}
}
@book{Gertler2011,
abstract = {In this fi rst part of the book, we give an overview of what impact evaluation is about. In chapter 1, we discuss why impact evaluation is important and how it fi ts within the context of evidence-based policy making. We contrast impact evaluation with other common evaluation practices, such as monitor- ing and process evaluations. Finally, we introduce different modalities of im- pact evaluation, such as prospective and retrospective evaluation, and effi cacy versus effi ciency trials. In chapter 2, we discuss how to formulate evaluation questions and hypoth- eses that are useful for policy. These questions and hypotheses form the ba- sis of evaluation because they determine what it is that the evaluation will be looking},
author = {Gertler, Paul J and Martinez, Sebastian and Premand, Patrick and Rawlings, Laura B and Vermeersch, Christel M J},
booktitle = {The World Bank Publications},
doi = {10.1596/978-0-8213-8541-8},
isbn = {9780821385418},
pages = {266},
title = {{Impact Evaluation in Practice}},
url = {http://siteresources.worldbank.org/EXTHDOFFICE/Resources/5485726-1295455628620/Impact{\{}{\_}{\}}Evaluation{\{}{\_}{\}}in{\{}{\_}{\}}Practice.pdf},
year = {2011}
}
@book{Gertler2011,
abstract = {In this fi rst part of the book, we give an overview of what impact evaluation is about. In chapter 1, we discuss why impact evaluation is important and how it fi ts within the context of evidence-based policy making. We contrast impact evaluation with other common evaluation practices, such as monitor- ing and process evaluations. Finally, we introduce different modalities of im- pact evaluation, such as prospective and retrospective evaluation, and effi cacy versus effi ciency trials. In chapter 2, we discuss how to formulate evaluation questions and hypoth- eses that are useful for policy. These questions and hypotheses form the ba- sis of evaluation because they determine what it is that the evaluation will be looking},
author = {Gertler, Paul J and Martinez, Sebastian and Premand, Patrick and Rawlings, Laura B and Vermeersch, Christel M J},
booktitle = {The World Bank Publications},
doi = {10.1596/978-0-8213-8541-8},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gertler et al. - 2011 - Impact Evaluation in Practice.pdf:pdf},
isbn = {9780821385418},
pages = {266},
title = {{Impact Evaluation in Practice}},
url = {http://siteresources.worldbank.org/EXTHDOFFICE/Resources/5485726-1295455628620/Impact{\_}Evaluation{\_}in{\_}Practice.pdf},
year = {2011}
}
@misc{Ghemawat2003,
abstract = {We have designed and implemented the Google File System, a scalable distributed file system for large distributed data-intensive applications. It provides fault tolerance while running on inexpensive commodity hardware, and it delivers high aggregate performance to a large number of clients. While sharing many of the same goals as previous distributed file systems, our design has been driven by observations of our application workloads and technological environment, both current and anticipated, that reflect a marked departure from some earlier file system assumptions. This has led us to reexamine traditional choices and explore radically different design points. The file system has successfully met our storage needs. It is widely deployed within Google as the storage platform for the generation and processing of data used by our service as well as research and development efforts that require large data sets. The largest cluster to date provides hundreds of terabytes of storage across thousands of disks on over a thousand machines, and it is concurrently accessed by hundreds of clients. In this paper, we present file system interface extensions designed to support distributed applications, discuss many aspects of our design, and report measurements from both micro-benchmarks and real world use.},
author = {Ghemawat, Sanjay and Gobioff, Howard and Leung, Shun-Tak},
booktitle = {ACM SIGOPS Operating Systems Review},
number = {5},
pages = {29},
title = {{The Google file system}},
volume = {37},
year = {2003}
}
@book{GIBBONS,
address = {New York},
author = {Gibbons, J and Chakraborti, S},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gibbons - 1992 - Nonparametric statistical inference.pdf:pdf},
publisher = {Marcel Dekker},
title = {{Nonparametric Statistical Inference}},
year = {2003}
}
@book{GIBBONS,
address = {New York},
annote = {From Duplicate 1 (Nonparametric statistical inference - Gibbons, J D)

Sustenta el uso no param{\'{e}}trico},
author = {Gibbons, J D and Chakraborti, S},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gibbons - 1992 - Nonparametric statistical inference.pdf:pdf},
isbn = {0824740521},
keywords = {Methods},
publisher = {Marcel Dekker},
title = {{Nonparametric Statistical Inference}},
year = {2003}
}
@article{GORDON,
annote = {From Duplicate 2 (A Review of Hierarchical Clasification - Gordon, A D)

Una revisi{\'{o}}n sobre clasificaci{\'{o}}n jer{\'{a}}rquica.},
author = {Gordon, A D},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gordon - 1987 - A Review of Hierarchical Clasification.pdf:pdf},
isbn = {0000000779},
journal = {Journal of the Royal Statistical Society},
keywords = {clinical trials,equivalence,ethics,power,prediction,prior distribution,range of,shrinkage,stopping rules,subjective probabilities},
number = {2},
pages = {119--137},
title = {{A Review of Hierarchical Clasification}},
volume = {150},
year = {1987}
}
@article{Gordon1987,
abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. SUMMARY A review is presented of methods of summarizing the relationships within a set of objects by a set of hierarchically-nested classes of similar objects, representable by a rooted tree diagram. Material covered includes algorithms for obtaining tree diagrams, comments on the selection of appropriate methods of analysis and the validation of classifications, distributions of different types of tree, and consensus trees.},
author = {Gordon, A D and Gordont, A D},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gordon, Gordont - 1987 - A Review of Hierarchical Classification.pdf:pdf},
journal = {Source Journal of the Royal Statistical Society. Series A (General) J. R. Statist. Soc. A},
keywords = {CLASSIFICATION,CLUSTER ANALYSIS,CONSENSUS TREES,DENDROGRAMS,HIERARCHICAL CLASSIFICATION,N-TREES,ULTRAMETRIC INEQUALITY,VALIDATION},
number = {2},
pages = {119--137},
title = {{A Review of Hierarchical Classification}},
url = {http://www.jstor.org http://www.jstor.org/stable/2981629 http://www.jstor.org/page/},
volume = {150},
year = {1987}
}
@article{AG01,
author = {Guti{\'{e}}rrez, A and Zhang, H},
journal = {Comunicaciones en Estad$\backslash$'istica. Universidad Santo Tom{\'{a}}s},
pages = {55--61},
title = {{Dos expresiones para la esperanza de una variable aleatoria}},
volume = {1},
year = {2008}
}
@article{HANIF,
author = {Hanif, Muhammad},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hanif - 2011 - Reweighted Nadaraya-Watson estimator of scalar diffusion models by using asymmetric kernels.pdf:pdf},
journal = {Far East Journal of Psychology and Business},
keywords = {beta kernel,estimation,gamma kernel,harris recurrence,local time,nonparametric,paper type,research paper,reweighted nadaraya-watson estimator,stochastic differential equation},
number = {1},
pages = {53--69},
title = {{Reweighted Nadaraya-Watson estimator of scalar diffusion models by using asymmetric kernels}},
volume = {4},
year = {2011}
}
@article{Hanif2011,
author = {Hanif, Muhammad},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hanif - 2011 - Reweighted Nadaraya-Watson estimator of scalar diffusion models by using asymmetric kernels.pdf:pdf},
keywords = {beta kernel,estimation,gamma kernel,harris recurrence,local time,nonparametric,paper type,research paper,reweighted nadaraya-watson estimator,stochastic differential equation},
number = {1},
pages = {53--70},
title = {{Reweighted Nadaraya-Watson estimator of scalar diffusion models by using asymmetric kernels}},
volume = {4},
year = {2011}
}
@article{Hannah-Shmouni2015,
author = {Hannah-Shmouni, Fady and Seidelmann, Sara B and Sirrs, Sandra and Mani, Arya and Jacoby, Daniel},
doi = {10.1016/j.cjca.2015.07.735},
issn = {0828282X},
journal = {Canadian Journal of Cardiology},
number = {11},
pages = {1338--1350},
publisher = {Elsevier Ltd},
title = {{The Genetic Challenges and Opportunities in Advanced Heart Failure}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0828282X15013161},
volume = {31},
year = {2015}
}
@article{HARTIGAN,
annote = {From Duplicate 1 (A K - Means Clustering Algorithm - Hartigan, J A; Wong, M A)

Algoritmo de K-Means.},
author = {Hartigan, J A and Wong, M A},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hartigan, Wong - 1979 - A K - Means Clustering Algorithm.pdf:pdf},
isbn = {0000000779},
journal = {Journal of the Royal Statistical Society},
keywords = {clinical trials,equivalence,ethics,power,prediction,prior distribution,range of,shrinkage,stopping rules,subjective probabilities},
number = {1},
pages = {100--108},
title = {{A K - Means Clustering Algorithm}},
volume = {28},
year = {1979}
}
@techreport{EPMEANS,
author = {Henderson, Keith and Gallagher, Brian and Eliassi-rad, Tina},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Henderson, Gallagher, Eliassi-rad - 2015 - EP-MEANS An Efficient Nonparametric Clustering of Empirical Probability Distributions.pdf:pdf},
isbn = {9781450331968},
title = {{EP-MEANS: An Efficient Nonparametric Clustering of Empirical Probability Distributions}},
year = {2015}
}
@techreport{Henderson2015,
author = {Henderson, Keith and Gallagher, Brian and Eliassi-rad, Tina},
isbn = {9781450331968},
title = {{EP-MEANS: An Efficient Nonparametric Clustering of Empirical Probability Distributions}},
year = {2015}
}
@article{HC,
author = {Henderson, R and Clark, K},
journal = {Administrative Science Quarterly},
pages = {9--30},
title = {{Architectural Innovation: The Reconfiguration of Existing Product Technologies and the Failure of Established Firms}},
volume = {35},
year = {1990}
}
@book{HernandezSampieri2010,
abstract = {Esta quinta edici{\'{o}}n conserva su car{\'{a}}cter did{\'{a}}ctico y multidisciplinario, pero expande sus perspectivas, ya que se ha convertido en un libro interactivo que vincula el contenido del texto ; de tal manera que la parte impresa resulta ahora m{\'{a}}s manejable para el lector ; asi como distintos ejemplos cuantitativos, cualitativos y de estudios mixtos. Por ello, puede utilizarse en cursos b{\'{a}}sicos de unvestigaci{\'{o}}n, pero tambi{\'{e}}n en asignaturas de nivel intermedio y avanzado.},
author = {{Hernandez Sampieri}, Roberto and {Fernandez Collado}, Carlos and {Baptista Lucio}, Maria del Pilar},
booktitle = {McGraw Hill},
isbn = {9786071502919},
title = {{Metodolog{\'{i}}a de la investigaci{\'{o}}n}},
url = {http://www.casadellibro.com/libro-metodologia-de-la-investigacion-5-ed-incluye-cd-rom/9786071502919/1960006},
year = {2010}
}
@book{HernandezSampieri2010,
abstract = {Esta quinta edici{\{}{\'{o}}{\}}n conserva su car{\{}{\'{a}}{\}}cter did{\{}{\'{a}}{\}}ctico y multidisciplinario, pero expande sus perspectivas, ya que se ha convertido en un libro interactivo que vincula el contenido del texto ; de tal manera que la parte impresa resulta ahora m{\{}{\'{a}}{\}}s manejable para el lector ; asi como distintos ejemplos cuantitativos, cualitativos y de estudios mixtos. Por ello, puede utilizarse en cursos b{\{}{\'{a}}{\}}sicos de unvestigaci{\{}{\'{o}}{\}}n, pero tambi{\{}{\'{e}}{\}}n en asignaturas de nivel intermedio y avanzado.},
author = {{Hernandez Sampieri}, Roberto and {Fernandez Collado}, Carlos and {Baptista Lucio}, Maria del Pilar},
booktitle = {McGraw Hill},
isbn = {9786071502919},
title = {{Metodolog{\{}{\'{i}}{\}}a de la investigaci{\{}{\'{o}}{\}}n}},
url = {http://www.casadellibro.com/libro-metodologia-de-la-investigacion-5-ed-incluye-cd-rom/9786071502919/1960006},
year = {2010}
}
@book{ILLIAN,
address = {London},
author = {Illian, J and Penttinen, A and Stoyan, H and Stoyan, D},
publisher = {John Wiley {\&} Sons},
title = {{Statistical Analysis and Modelling of Spatial Point Patterns}},
year = {2008}
}
@article{James,
abstract = {We develop a flexible model-based procedure for clustering functional data. The technique can be applied to all types of curve data but is particularly useful when individuals are observed at a sparse set of time points. In addition to producing final cluster assignments, the procedure generates predictions and confidence intervals for missing portions of curves. Our approach also provides many useful tools for evaluating the resulting models. Clustering can be assessed visually via low dimensional represen- tations of the curves, and the regions of greatest separation between clusters can be determined using a discriminant function. Finally, we extend the model to handle multiple functional and finite dimensional covariates and show how it can be applied to standard finite dimensional clustering problems involving missing data.},
author = {James, Gareth M and Sugar, Catherine A},
doi = {10.1198/016214503000189},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/James, Sugar - 2003 - Clustering for Sparsely Sampled Functional Data.pdf:pdf},
isbn = {0001689117171},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {Curve estimation,Discriminant functions,Functional clustering,High dimensional data,Some key words},
number = {462},
pages = {397--408},
title = {{Clustering for Sparsely Sampled Functional Data}},
volume = {98},
year = {2003}
}
@article{Javier2015,
author = {Javier, Carlos and Causil, Barrera},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Javier, Causil - 2015 - Analysis of the elicited prior distributions using tools of functional data analysis.pdf:pdf},
title = {{Analysis of the elicited prior distributions using tools of functional data analysis}},
year = {2015}
}
@article{Jiang2004,
author = {Jiang, D and Tang, C and Zhang, a},
journal = {IEEE Transactions on Knowledge and Data},
number = {11},
pages = {1370--1386},
title = {{Cluster analysis for gene expression data: A survey}},
url = {http://scholar.google.com/scholar?hl=en{\{}{\{}{\}}{\{}{\&}{\}}{\{}{\}}{\}}btnG=Search{\{}{\{}{\}}{\{}{\&}{\}}{\{}{\}}{\}}q=intitle:Cluster+Analysis+for+Gene+Expression+Data+A+Survey{\{}{\{}{\}}{\{}{\#}{\}}{\{}{\}}{\}}0},
volume = {16},
year = {2004}
}
@article{Jiang2004,
author = {Jiang, D and Tang, C and Zhang, a},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiang, Tang, Zhang - 2004 - Cluster analysis for gene expression data A survey.pdf:pdf},
journal = {IEEE Transactions on Knowledge and Data},
number = {11},
pages = {1370--1386},
title = {{Cluster analysis for gene expression data: A survey}},
url = {http://scholar.google.com/scholar?hl=en{\{}{\&}{\}}btnG=Search{\{}{\&}{\}}q=intitle:Cluster+Analysis+for+Gene+Expression+Data+A+Survey{\{}{\#}{\}}0},
volume = {16},
year = {2004}
}
@article{Joe1963,
annote = {Algoritmo jer{\'{a}}rquico de Ward.},
author = {Joe, H and Ward, Jr},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Joe, Ward - 1963 - Hierarchical Grouping to Optimize an Objective Function.pdf:pdf},
journal = {Journal of the American Statistical Association1},
number = {301},
pages = {236 -- 244},
title = {{Hierarchical Grouping to Optimize an Objective Function}},
volume = {58},
year = {1963}
}
@book{Jolliffe2002,
abstract = {Principal component analysis is central to the study of multivariate data. Although one of the earliest multivariate techniques it continues to be the subject of much research, ranging from new model- based approaches to algorithmic ideas from neural networks. It is extremely versatile with applications in many disciplines. The first edition of this book was the first comprehensive text written solely on principal component analysis. The second edition updates and substantially expands the original version, and is once again the definitive text on the subject. It includes core material, current research and a wide range of applications. Its length is nearly double that of the first edition. Researchers in statistics, or in other fields that use principal component analysis, will find that the book gives an authoritative yet accessible account of the subject. It is also a valuable resource for graduate courses in multivariate analysis. The book requires some knowledge of matrix algebra. Ian Jolliffe is Professor of Statistics at the University of Aberdeen. He is author or co-author of over 60 research papers and three other books. His research interests are broad, but aspects of principal component analysis have fascinated him and kept him busy for over 30 years.},
author = {Jolliffe, I T},
booktitle = {Encyclopedia of Statistics in Behavioral Science},
doi = {10.2307/1270093},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jolliffe - 2002 - Principal Component Analysis, Second Edition.pdf:pdf},
isbn = {0387954422},
issn = {00401706},
number = {3},
pages = {487},
pmid = {21435900},
title = {{Principal Component Analysis, Second Edition}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/0470013192.bsa501/full},
volume = {30},
year = {2002}
}
@book{Jolliffe2002,
abstract = {Principal component analysis is central to the study of multivariate data. Although one of the earliest multivariate techniques it continues to be the subject of much research, ranging from new model- based approaches to algorithmic ideas from neural networks. It is extremely versatile with applications in many disciplines. The first edition of this book was the first comprehensive text written solely on principal component analysis. The second edition updates and substantially expands the original version, and is once again the definitive text on the subject. It includes core material, current research and a wide range of applications. Its length is nearly double that of the first edition. Researchers in statistics, or in other fields that use principal component analysis, will find that the book gives an authoritative yet accessible account of the subject. It is also a valuable resource for graduate courses in multivariate analysis. The book requires some knowledge of matrix algebra. Ian Jolliffe is Professor of Statistics at the University of Aberdeen. He is author or co-author of over 60 research papers and three other books. His research interests are broad, but aspects of principal component analysis have fascinated him and kept him busy for over 30 years.},
author = {Jolliffe, I T},
booktitle = {Encyclopedia of Statistics in Behavioral Science},
doi = {10.2307/1270093},
isbn = {0387954422},
issn = {00401706},
number = {3},
pages = {487},
pmid = {21435900},
title = {{Principal Component Analysis, Second Edition}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/0470013192.bsa501/full},
volume = {30},
year = {2002}
}
@book{Jolliffe2002,
abstract = {Principal component analysis is central to the study of multivariate data. Although one of the earliest multivariate techniques it continues to be the subject of much research, ranging from new model- based approaches to algorithmic ideas from neural networks. It is extremely versatile with applications in many disciplines. The first edition of this book was the first comprehensive text written solely on principal component analysis. The second edition updates and substantially expands the original version, and is once again the definitive text on the subject. It includes core material, current research and a wide range of applications. Its length is nearly double that of the first edition. Researchers in statistics, or in other fields that use principal component analysis, will find that the book gives an authoritative yet accessible account of the subject. It is also a valuable resource for graduate courses in multivariate analysis. The book requires some knowledge of matrix algebra. Ian Jolliffe is Professor of Statistics at the University of Aberdeen. He is author or co-author of over 60 research papers and three other books. His research interests are broad, but aspects of principal component analysis have fascinated him and kept him busy for over 30 years.},
author = {Jolliffe, I T},
booktitle = {Encyclopedia of Statistics in Behavioral Science},
doi = {10.2307/1270093},
isbn = {0387954422},
issn = {00401706},
number = {3},
pages = {487},
pmid = {21435900},
title = {{Principal Component Analysis, Second Edition}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/0470013192.bsa501/full},
volume = {30},
year = {2002}
}
@book{Kalton1983,
author = {Kalton, Graham},
booktitle = {Sage university papers series. Quantitative applications in the social sciences ; no. 07-035},
isbn = {0803921268 (pbk.)},
keywords = {Sampling (Statistics),Sampling Statistics,Sampling Studies methods,Social Sciences methods,Social Sciences statistics {\&} numerical data,Social sciences Methodology,Social sciences Statistical methods,Social surveys Sampling,Statistics,Survey sampling},
pages = {96},
title = {{Introduction to survey sampling}},
url = {http://www.amazon.com/dp/0803921268},
year = {1983}
}
@book{Kalton1983,
author = {Kalton, Graham},
booktitle = {Sage university papers series. Quantitative applications in the social sciences ; no. 07-035},
isbn = {0803921268 (pbk.)},
keywords = {Sampling (Statistics),Sampling Statistics,Sampling Studies methods,Social Sciences methods,Social Sciences statistics {\{}{\&}{\}} numerical data,Social sciences Methodology,Social sciences Statistical methods,Social surveys Sampling,Statistics,Survey sampling},
pages = {96},
title = {{Introduction to survey sampling}},
url = {http://www.amazon.com/dp/0803921268},
year = {1983}
}
@article{Karypis1999,
abstract = {Clustering is a discovery process in data mining. It groups a set of data in a way that maximizes the similarity within clusters and minimizes the similarity between two different clusters. Many advanced algorithms have difficulty dealing with highly variable clusters that do not follow a preconceived model. By basing its selections on both interconnectivity and closeness, the Chameleon algorithm yields accurate results for these highly variable clusters. Existing algorithms use a static model of the clusters and do not use information about the nature of individual clusters as they are merged. Furthermore, one set of schemes (the CURE algorithm and related schemes) ignores the information about the aggregate interconnectivity of items in two clusters. Another set of schemes (the Rock algorithm, group averaging method, and related schemes) ignores information about the closeness of two clusters as defined by the similarity of the closest items across two clusters. By considering either interconnectivity or closeness only, these algorithms can select and merge the wrong pair of clusters. Chameleon's key feature is that it accounts for both interconnectivity and closeness in identifying the most similar pair of clusters. Chameleon finds the clusters in the data set by using a two-phase algorithm. During the first phase, Chameleon uses a graph partitioning algorithm to cluster the data items into several relatively small subclusters. During the second phase, it uses an algorithm to find the genuine clusters by repeatedly combining these subclusters},
author = {Karypis, George and Han, Eui-Hong and Kumar, Vipin},
doi = {10.1109/2.781637},
isbn = {0018-9162 VO - 32},
issn = {00189162},
journal = {Computer},
keywords = {Aggregates,CURE algorithm,Chameleon algorithm,Clustering algorithms,Earthquakes,Extraterrestrial measurements,Proteins,Rock algorithm,Seismology,Shape,advanced algorithms,aggregate interconnectivity,closeness,closest items,data analysis,data item clustering,data mining,data set,discovery process,dynamic modeling,graph partitioning algorithm,graph theory,hierarchical clustering,highly variable clusters,most similar pair,pattern clustering,subclusters,two-phase algorithm},
number = {8},
pages = {68--75},
pmid = {781637},
title = {{Chameleon: hierarchical clustering using dynamic modeling}},
volume = {32},
year = {1999}
}
@article{Karypis1999,
abstract = {Clustering is a discovery process in data mining. It groups a set of data in a way that maximizes the similarity within clusters and minimizes the similarity between two different clusters. Many advanced algorithms have difficulty dealing with highly variable clusters that do not follow a preconceived model. By basing its selections on both interconnectivity and closeness, the Chameleon algorithm yields accurate results for these highly variable clusters. Existing algorithms use a static model of the clusters and do not use information about the nature of individual clusters as they are merged. Furthermore, one set of schemes (the CURE algorithm and related schemes) ignores the information about the aggregate interconnectivity of items in two clusters. Another set of schemes (the Rock algorithm, group averaging method, and related schemes) ignores information about the closeness of two clusters as defined by the similarity of the closest items across two clusters. By considering either interconnectivity or closeness only, these algorithms can select and merge the wrong pair of clusters. Chameleon's key feature is that it accounts for both interconnectivity and closeness in identifying the most similar pair of clusters. Chameleon finds the clusters in the data set by using a two-phase algorithm. During the first phase, Chameleon uses a graph partitioning algorithm to cluster the data items into several relatively small subclusters. During the second phase, it uses an algorithm to find the genuine clusters by repeatedly combining these subclusters},
author = {Karypis, George and Han, Eui-Hong and Kumar, Vipin},
doi = {10.1109/2.781637},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Karypis, Han, Kumar - 1999 - Chameleon hierarchical clustering using dynamic modeling.pdf:pdf},
isbn = {0018-9162 VO - 32},
issn = {00189162},
journal = {Computer},
keywords = {Aggregates,CURE algorithm,Chameleon algorithm,Clustering algorithms,Earthquakes,Extraterrestrial measurements,Proteins,Rock algorithm,Seismology,Shape,advanced algorithms,aggregate interconnectivity,closeness,closest items,data analysis,data item clustering,data mining,data set,discovery process,dynamic modeling,graph partitioning algorithm,graph theory,hierarchical clustering,highly variable clusters,most similar pair,pattern clustering,subclusters,two-phase algorithm},
number = {8},
pages = {68--75},
pmid = {781637},
title = {{Chameleon: hierarchical clustering using dynamic modeling}},
volume = {32},
year = {1999}
}
@article{Kashyap2015,
abstract = {Bioinformatics research is characterized by voluminous and incremental datasets and complex data analytics methods. The machine learning methods used in bioinformatics are iterative and parallel. These methods can be scaled to handle big data using the distributed and parallel computing technologies. Usually big data tools perform computation in batch-mode and are not optimized for iterative processing and high data dependency among operations. In the recent years, parallel, incremental, and multi-view machine learning algorithms have been proposed. Similarly, graph-based architectures and in-memory big data tools have been developed to minimize I/O cost and optimize iterative processing. However, there lack standard big data architectures and tools for many important bioinformatics problems, such as fast construction of co-expression and regulatory networks and salient module identification, detection of complexes over growing protein-protein interaction data, fast analysis of massive DNA, RNA, and protein sequence data, and fast querying on incremental and heterogeneous disease networks. This paper addresses the issues and challenges posed by several big data problems in bioinformatics, and gives an overview of the state of the art and the future research opportunities.},
archivePrefix = {arXiv},
arxivId = {1506.05101},
author = {Kashyap, Hirak and Ahmed, Hasin Afzal and Hoque, Nazrul and Roy, Swarup and Bhattacharyya, Dhruba Kumar},
eprint = {1506.05101},
number = {9},
pages = {1--20},
title = {{Big Data Analytics in Bioinformatics: A Machine Learning Perspective}},
url = {http://arxiv.org/abs/1506.05101},
volume = {13},
year = {2015}
}
@article{Kloke2014,
author = {Kloke, John D and Mckean, Joseph W},
pages = {1--263},
title = {{Nonparametric Statistical Methods Using R}},
year = {2014}
}
@article{Kloke2014,
author = {Kloke, John D and Mckean, Joseph W},
pages = {1--263},
title = {{Nonparametric Statistical Methods Using R}},
year = {2014}
}
@book{KOLMOGOROV,
address = {New York},
author = {Kolmogorov, A N},
publisher = {Chelsea Publishing Company},
title = {{Foundations of the Theory of Probability}},
year = {1956}
}
@article{KWTEST,
annote = {From Duplicate 2 (Use of Ranks in One-Criterion Variance Analysis - Kruskal, William H; Wallis, W. Allen)

Prueba de Kruskall Wallis},
author = {Kruskal, W and Wallis, A},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kruskal, Wallis - 1952 - Use of Ranks in One-Criterion Variance Analysis.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {260},
pages = {583--621},
title = {{Use of Ranks in One-Criterion Variance Analysis}},
volume = {47},
year = {1952}
}
@article{Krzanowsky1987,
annote = {No s{\'{e}} para qu{\'{e}} pueda servir.},
author = {Krzanowsky, W J},
doi = {10.1002/0471667196.ess0146.pub2},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krzanowsky - 1987 - Cross - Validation in Principal Component Analysis.pdf:pdf},
isbn = {9780471667193},
journal = {Biometrics},
number = {3},
pages = {575 -- 584},
title = {{Cross - Validation in Principal Component Analysis}},
volume = {43},
year = {1987}
}
@article{Kumari2014,
author = {Kumari, D Aruna and Bhavana, D Poojitha and Aditya, V Venkata Sai},
number = {9},
pages = {1--3},
title = {{Data Mining in Biodata Analysis}},
volume = {14},
year = {2014}
}
@article{La2012,
abstract = {Sirve como ejercicio para que no se te olvide como hacer esto},
author = {La, P},
doi = {10.5867/medwave.2003.11.2757},
isbn = {8707955499},
issn = {07176384},
pages = {10--12},
title = {{Ejercicio 1}},
year = {2012}
}
@article{La2012,
abstract = {Sirve como ejercicio para que no se te olvide como hacer esto},
author = {La, P},
doi = {10.5867/medwave.2003.11.2757},
isbn = {8707955499},
issn = {07176384},
pages = {10--12},
title = {{Ejercicio 1}},
year = {2012}
}
@book{LEBART,
address = {Paris},
author = {Lebart, L and Morineau, A and Piron, M},
publisher = {Dunod},
title = {{Statistique exploratorie multidimensionelle}},
year = {1995}
}
@book{LEBART,
address = {Paris},
author = {Lebart, L and Morineau, A and Piron, M},
publisher = {Dunod},
title = {{Statistique exploratorie multidimensionelle}},
year = {1995}
}
@article{Leon2010,
abstract = {In this paper, we present a scalable evolutionary algorithm for clustering large and dynamic data sets, called Scalable Evolutionary Clustering with Self Adaptive Genetic Operators (Scalable ECSAGO). The proposed evolutionary clustering algorithm can adapt its genetic operators rate while the evolution leads to the optimal centers of the clusters. The sizes of the clusters are estimated using a hybrid analytical optimization procedure. Moreover, a memorization factor is introduced in order to allow the algorithm to keep as much of the previously discovered knowledge about clusters and data summarization as desired. The proposed scalable ECSAGO algorithm is able to find accurate representations of the clusters on very large data sets of different sizes and dimensionality that might not fit in main memory, while maintaining the desirable properties of robustness to noise and automatic detection of the number of clusters. The algorithm is also useful for traking evolving cluster structures that change with the passage of time.},
annote = {Agrupaci{\'{o}}n basada en algoritmos gen{\'{e}}ticos.

Leer detenidamente y ver si se puede aplicar.},
author = {Le{\'{o}}n, Elizabeth and Nasraoui, Olfa and Gomez, Jonatan},
doi = {10.1109/CEC.2010.5586467},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Le{\'{o}}n, Nasraoui, Gomez - 2010 - Scalable evolutionary clustering algorithm with self adaptive genetic operators.pdf:pdf},
isbn = {9781424469109},
journal = {2010 IEEE World Congress on Computational Intelligence, WCCI 2010 - 2010 IEEE Congress on Evolutionary Computation, CEC 2010},
title = {{Scalable evolutionary clustering algorithm with self adaptive genetic operators}},
year = {2010}
}
@article{Leon2010,
abstract = {In this paper, we present a scalable evolutionary algorithm for clustering large and dynamic data sets, called Scalable Evolutionary Clustering with Self Adaptive Genetic Operators (Scalable ECSAGO). The proposed evolutionary clustering algorithm can adapt its genetic operators rate while the evolution leads to the optimal centers of the clusters. The sizes of the clusters are estimated using a hybrid analytical optimization procedure. Moreover, a memorization factor is introduced in order to allow the algorithm to keep as much of the previously discovered knowledge about clusters and data summarization as desired. The proposed scalable ECSAGO algorithm is able to find accurate representations of the clusters on very large data sets of different sizes and dimensionality that might not fit in main memory, while maintaining the desirable properties of robustness to noise and automatic detection of the number of clusters. The algorithm is also useful for traking evolving cluster structures that change with the passage of time.},
annote = {Agrupaci{\{}{\'{o}}{\}}n basada en algoritmos gen{\{}{\'{e}}{\}}ticos.

Leer detenidamente y ver si se puede aplicar.},
author = {Le{\'{o}}n, Elizabeth and Nasraoui, Olfa and Gomez, Jonatan},
doi = {10.1109/CEC.2010.5586467},
isbn = {9781424469109},
journal = {2010 IEEE World Congress on Computational Intelligence, WCCI 2010 - 2010 IEEE Congress on Evolutionary Computation, CEC 2010},
title = {{Scalable evolutionary clustering algorithm with self adaptive genetic operators}},
year = {2010}
}
@article{Liu2014,
abstract = {Discriminative pattern mining is one of the most important techniques in data mining. This challenging task is concerned with finding a set of patterns that occur with disproportionate frequency in data sets with various class labels. Such patterns are of great value for group difference detection and classifier construction. Research on finding interesting discriminative patterns in class-labeled data evolves rapidly and lots of algorithms have been proposed to specifically address this problem. Discriminative pattern mining techniques have proven their considerable value in biological data analysis. The archetypical applications in bioinformatics include phosphorylation motif discovery, differentially expressed gene identification, discriminative genotype pattern detection, etc. In this article, we present an overview of discriminative pattern mining and the corresponding effective methods, and subsequently we illustrate their applications to tackling the bioinformatics problems. In the end, we give a general discussion of potential challenges and future work for this task.},
author = {Liu, Xiaoqing and Wu, Jun and Gu, Feiyang and Wang, Jie and He, Zengyou},
doi = {10.1093/bib/bbu042},
issn = {1467-5463},
journal = {Briefings in Bioinformatics},
keywords = {contrast sets,discriminative pattern mining,emerging patterns,subgroup discovery},
number = {October},
pages = {1--17},
title = {{Discriminative pattern mining and its applications in bioinformatics}},
url = {http://bib.oxfordjournals.org/cgi/doi/10.1093/bib/bbu042},
year = {2014}
}
@article{Li2014,
author = {Li, Yixue and Chen, Luonan},
doi = {10.1016/j.gpb.2014.10.001},
issn = {2210-3244},
journal = {Genomics, proteomics {\{}{\&}{\}} bioinformatics},
keywords = {Computational Biology,Computational Biology: methods,Data Mining,Data Mining: methods,Gene Expression Profiling,High-Throughput Screening Assays,Humans,Software},
number = {5},
pages = {187--189},
pmid = {25462151},
publisher = {Beijing Institute of Genomics, Chinese Academy of Sciences and Genetics Society of China},
title = {{Big biological data: challenges and opportunities.}},
url = {http://www.sciencedirect.com/science/article/pii/S1672022914001041},
volume = {12},
year = {2014}
}
@article{Lowe2004,
abstract = {This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.},
author = {Lowe, David G.},
journal = {International Journal of Computer Vision},
keywords = {Image matching,Invariant features,Object recognition,Scale invariance},
number = {2},
pages = {91--110},
title = {{Distinctive image features from scale-invariant keypoints}},
volume = {60},
year = {2004}
}
@article{Lowe1999,
abstract = {An object recognition system has been developed that uses a new
class of local image features. The features are invariant to image
scaling, translation, and rotation, and partially invariant to
illumination changes and affine or 3D projection. These features share
similar properties with neurons in inferior temporal cortex that are
used for object recognition in primate vision. Features are efficiently
detected through a staged filtering approach that identifies stable
points in scale space. Image keys are created that allow for local
geometric deformations by representing blurred image gradients in
multiple orientation planes and at multiple scales. The keys are used as
input to a nearest neighbor indexing method that identifies candidate
object matches. Final verification of each match is achieved by finding
a low residual least squares solution for the unknown model parameters.
Experimental results show that robust object recognition can be achieved
in cluttered partially occluded images with a computation time of under
2 seconds},
author = {Lowe, D.G.},
journal = {Proceedings of the Seventh IEEE International Conference on Computer Vision},
title = {{Object recognition from local scale-invariant features}},
volume = {2},
year = {1999}
}
@article{Maharjan2011,
author = {Maharjan, Merina},
pages = {1--23},
title = {{Genome Analysis with MapReduce}},
year = {2011}
}
@misc{Manning2009,
abstract = {Class-tested and coherent, this groundbreaking new textbook teaches web-era information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. Written from a computer science perspective by three leading experts in the field, it gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Although originally designed as the primary text for a graduate or advanced undergraduate course in information retrieval, the book will also create a buzz for researchers and professionals alike.},
archivePrefix = {arXiv},
arxivId = {0521865719 9780521865715},
author = {Manning, Christopher D. and Raghavan, Prabhakar},
booktitle = {Online},
chapter = {Boolean Re},
doi = {10.1109/LPT.2009.2020494},
editor = {Salas, A Cannon-Bowers E},
eprint = {0521865719 9780521865715},
institution = {Cambridge University Press Cambridge, England},
isbn = {0521865719},
issn = {13864564},
keywords = {keyword},
pages = {1},
pmid = {10575050},
publisher = {Cambridge University Press},
title = {{An Introduction to Information Retrieval}},
url = {http://dspace.cusat.ac.in/dspace/handle/123456789/2538},
year = {2009}
}
@book{MarioF.TRiola2009,
author = {{Mario F. TRiola}},
isbn = {9789702612872},
title = {{Estad{\'{i}}sticas}},
year = {2009}
}
@book{MarioF.TRiola2009,
author = {{Mario F. TRiola}},
isbn = {9789702612872},
title = {{Estad{\{}{\'{i}}{\}}sticas}},
year = {2009}
}
@article{KS,
author = {{Massey Jr}, F J},
journal = {Journal of the American Statistical Association},
pages = {68--78},
title = {{The Kolmogorov-Smirnov Test for Goodness of Fit}},
volume = {46(253)},
year = {1951}
}
@book{EM,
abstract = {The only single-source--now completely updated and revised--to offer a unified treatment of the theory, methodology, and applications of the EM algorithm Complete with updates that capture developments from the past decade, The EM Algorithm and Extensions, Second Edition successfully provides a basic understanding of the EM algorithm by describing its inception, implementation, and applicability in numerous statistical contexts. In conjunction with the fundamentals of the topic, the authors discuss convergence issues and computation of standard errors, and, in addition, unveil many parallels and connections between the EM algorithm and Markov chain Monte Carlo algorithms. Thorough discussions on the complexities and drawbacks that arise from the basic EM algorithm, such as slow convergence and lack of an in-built procedure to compute the covariance matrix of parameter estimates, are also presented. While the general philosophy of the First Edition has been maintained, this timely new edition has been updated, revised, and expanded to include: The EM Algorithm and Extensions, Second Edition serves as an excellent text for graduate-level statistics students and is also a comprehensive resource for theoreticians, practitioners, and researchers in the social and physical sciences who would like to extend their knowledge of the EM algorithm.},
author = {Mclachlan, Geoffrey J and Krishnan, Thriyambakam},
doi = {10.1002/9780470191613},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mclachlan, Krishnan - 2008 - The EM Algorithm and Extensions Second Edition.pdf:pdf},
isbn = {9780471201700},
title = {{The EM Algorithm and Extensions Second Edition}},
year = {2008}
}
@book{EM,
abstract = {The only single-source--now completely updated and revised--to offer a unified treatment of the theory, methodology, and applications of the EM algorithm Complete with updates that capture developments from the past decade, The EM Algorithm and Extensions, Second Edition successfully provides a basic understanding of the EM algorithm by describing its inception, implementation, and applicability in numerous statistical contexts. In conjunction with the fundamentals of the topic, the authors discuss convergence issues and computation of standard errors, and, in addition, unveil many parallels and connections between the EM algorithm and Markov chain Monte Carlo algorithms. Thorough discussions on the complexities and drawbacks that arise from the basic EM algorithm, such as slow convergence and lack of an in-built procedure to compute the covariance matrix of parameter estimates, are also presented. While the general philosophy of the First Edition has been maintained, this timely new edition has been updated, revised, and expanded to include: The EM Algorithm and Extensions, Second Edition serves as an excellent text for graduate-level statistics students and is also a comprehensive resource for theoreticians, practitioners, and researchers in the social and physical sciences who would like to extend their knowledge of the EM algorithm.},
author = {Mclachlan, Geoffrey J and Krishnan, Thriyambakam},
doi = {10.1002/9780470191613},
isbn = {9780471201700},
title = {{The EM Algorithm and Extensions Second Edition}},
year = {2008}
}
@book{MINARRO,
address = {Barcelona},
author = {Mi{\~{n}}arro, A},
title = {{Estimaci{\'{o}}n no param{\'{e}}trica de la funci{\'{o}}n de densidad}},
year = {1998}
}
@book{MINARRO,
address = {Barcelona},
author = {Mi{\~{n}}arro, A},
title = {{Estimaci{\{}{\'{o}}{\}}n no param{\{}{\'{e}}{\}}trica de la funci{\{}{\'{o}}{\}}n de densidad}},
year = {1998}
}
@article{Mirashe2010,
abstract = {Computing as you know it is about to change, your applications and documents are going to move from the desktop into the cloud. I'm talking about cloud computing, where applications and files are hosted on a "cloud" consisting of thousands of computers and servers, all linked together and accessible via the Internet. With cloud computing, everything you do is now web based instead of being desktop based. You can access all your programs and documents from any computer that's connected to the Internet. How will cloud computing change the way you work? For one thing, you're no longer tied to a single computer. You can take your work anywhere because it's always accessible via the web. In addition, cloud computing facilitates group collaboration, as all group members can access the same programs and documents from wherever they happen to be located. Cloud computing might sound far-fetched, but chances are you're already using some cloud applications. If you're using a web-based email program, such as Gmail or Hotmail, you're computing in the cloud. If you're using a web-based application such as Google Calendar or Apple Mobile Me, you're computing in the cloud. If you're using a file- or photo-sharing site, such as Flickr or Picasa Web Albums, you're computing in the cloud. It's the technology of the future, available to use today.},
archivePrefix = {arXiv},
arxivId = {1003.4074},
author = {Mirashe, Shivaji P and Kalyankar, N V},
chapter = {6},
doi = {10.1145/358438.349303},
editor = {Antonopoulos, Nick and Gillam, Lee},
eprint = {1003.4074},
institution = {Universit{\{}{\"{a}}{\}}t Stuttgart, Fakult{\{}{\"{a}}{\}}t Informatik, Elektrotechnik und Informationstechnik, Germany},
isbn = {1-58113-199-2},
issn = {03621340},
journal = {Communications of the ACM},
number = {7},
pages = {9},
pmid = {22988693},
publisher = {ACM},
series = {Informatik im Fokus},
title = {{Cloud Computing}},
url = {http://arxiv.org/abs/1003.4074},
volume = {51},
year = {2010}
}
@article{Mohammed2014,
abstract = {The emergence of massive datasets in a clinical setting presents both challenges and opportunities in data storage and analysis. This so called “big data” challenges traditional analytic tools and will increasingly require novel solutions adapted from other fields. Advances in information and communication technology present the most viable solutions to big data analysis in terms of efficiency and scalability. It is vital those big data solutions are multithreaded and that data access approaches be precisely tailored to large volumes of semi-structured/unstructured data. The MapReduce programming framework uses two tasks common in functional programming: Map and Reduce. MapReduce is a new parallel processing framework and Hadoop is its open-source implementation on a single computing node or on clusters. Compared with existing parallel processing paradigms (e.g. grid computing and graphical processing unit (GPU)), MapReduce and Hadoop have two advantages: 1) fault-tolerant storage resulting in reliable data processing by replicating the computing tasks, and cloning the data chunks on different computing nodes across the computing cluster; 2) high-throughput data processing via a batch processing framework and the Hadoop distributed file system (HDFS). Data are stored in the HDFS and made available to the slave nodes for computation. In this paper, we review the existing applications of the MapReduce programming framework and its implementation platform Hadoop in clinical big data and related medical health informatics fields. The usage of MapReduce and Hadoop on a distributed system represents a significant advance in clinical big data processing and utilization, and opens up new opportunities in the emerging era of big data analytics. The objective of this paper is to summarize the state-of-the-art efforts in clinical big data analytics and highlight what might be needed to enhance the outcomes of clinical big data analytics tools. This paper is concluded by summarizing the potential usage of the MapReduce programming framework and Hadoop platform to process huge volumes of clinical data in medical health informatics related fields.},
author = {Mohammed, Emad a. and Far, Behrouz H and Naugler, Christopher},
doi = {10.1186/1756-0381-7-22},
isbn = {1756-0381 (Linking)},
issn = {1756-0381},
journal = {BioData Mining},
keywords = {Big data,Bioinformatics,Clinical big data analysis,Clinical data analysis,Distributed programming,Hadoop,MapReduce},
number = {1},
pages = {1--23},
pmid = {25383096},
title = {{Applications of the MapReduce programming framework to clinical big data analysis: current landscape and future trends}},
url = {http://link.springer.com/article/10.1186/1756-0381-7-22$\backslash$nhttp://link.springer.com/content/pdf/10.1186/1756-0381-7-22.pdf},
volume = {7},
year = {2014}
}
@book{Montgomery2004,
address = {University of Arizona},
annote = {From Duplicate 2 (Dise{\{}{\~{n}}{\}}o y an{\{}{\'{a}}{\}}lisis de Experimentos - Montgomery, Douglas C.)

Sirve para sustentar los m{\{}{\'{e}}{\}}todos presentados},
author = {Montgomery, Douglas C},
isbn = {968-18-6156-6},
pages = {692},
publisher = {John Wiley {\{}{\&}{\}} Sons},
title = {{Dise{\{}{\~{n}}{\}}o y an{\{}{\'{a}}{\}}lisis de Experimentos}},
year = {2004}
}
@book{Montgomery2004,
address = {University of Arizona},
annote = {From Duplicate 2 (Dise{\~{n}}o y an{\'{a}}lisis de Experimentos - Montgomery, Douglas C.)

Sirve para sustentar los m{\'{e}}todos presentados},
author = {Montgomery, Douglas C.},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Montgomery - 2004 - Dise{\~{n}}o y an{\'{a}}lisis de Experimentos.pdf:pdf},
isbn = {968-18-6156-6},
pages = {692},
publisher = {John Wiley {\&} Sons},
title = {{Dise{\~{n}}o y an{\'{a}}lisis de Experimentos}},
year = {2004}
}
@article{Muller2006,
abstract = {We discuss Bayesian approaches to multiple comparison problems, using a decision theoretic perspective to critically compare competing approaches. We set up decision problems that lead to the use of FDR-based rules and generalizations. Alternative definitions of the probability model and the utility function lead to different rules and problem-specific adjustments. Using a loss function that controls realized FDR we derive an optimal Bayes rule that is a variation of the Benjamini and Hochberg (1995) procedure. The cutoff is based on increments in ordered posterior probabilities instead of ordered p- values. Throughout the discussion we take a Bayesian perspective. In particular, we focus on conditional expected FDR, conditional on the data. Variations of the probability model include explicit modeling for dependence. Variations of the utility function include weighting by the extent of a true negative and accounting for the impact in the final decision.},
annote = {FDR, leer y mirar cuidadosamente.},
author = {Muller, P and Parmigiani, Giovanni and Rice, Kenneth},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Muller, Parmigiani, Rice - 2006 - FDR and Bayesian multiple comparisons rules.pdf:pdf},
journal = {Bayesian Statistics 8},
keywords = {and phrases,decision problems,false,multiplicities},
number = {1995},
pages = {349--370},
title = {{FDR and Bayesian multiple comparisons rules}},
url = {http://biostats.bepress.com/jhubiostat/paper115/},
volume = {0},
year = {2006}
}
@article{Munzel2001,
abstract = {A general nonparametric approach to asymptotic multiple test procedures$\backslash$nis proposed which is based on relative effects and which includes$\backslash$ncontinuous as well as discontinuous distributions. The results can$\backslash$nbe applied to all relevant multiple testing problems in the one-way$\backslash$nlayout and include the well known Steel tests as special cases. Moreover,$\backslash$na general estimator for the asymptotic covariance matrix is considered$\backslash$nthat is consistent even under alternative. This estimator is used$\backslash$nto derive simultaneous confidence intervals for the relative effects$\backslash$nas well as a test procedure for the multiple nonparametric Behrens-Fisher$\backslash$nproblem.},
annote = {Mirar muy bien. Tiene paquete en R. Los ejemplos permiten ver que no piensa en muchos niveles para el factor.},
author = {Munzel, Ullrich and Hothorn, Ludwig a.},
doi = {10.1002/1521-4036(200109)43:5<553::AID-BIMJ553>3.0.CO;2-N},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Munzel, Hothorn - 2001 - A unified approach to simultaneous rank test procedures in the unbalanced one-way layout.pdf:pdf},
isbn = {0323-3847},
issn = {03233847},
journal = {Biometrical Journal},
keywords = {Behrens-Fisher problem,Multiple test procedure,Nonparametric,Simultaneous confidence intervals,Ties},
number = {5},
pages = {553--569},
title = {{A unified approach to simultaneous rank test procedures in the unbalanced one-way layout}},
volume = {43},
year = {2001}
}
@article{NADARAYA,
author = {Nadaraya, E},
journal = {Theory Probab. Appl.},
pages = {297--302},
title = {{On non nonparametric estimates for density functions and regression curves.}},
volume = {10},
year = {1965}
}
@article{Naulaerts,
abstract = {Over the past two decades, pattern mining techniques have become an integral part of many bioinformatics solu-tions. Frequent itemset mining is a popular group of pattern mining techniques designed to identify elements that frequently co-occur. An archetypical example is the identification of products that often end up together in the same shopping basket in supermarket transactions. A number of algorithms have been developed to address vari-ations of this computationally non-trivial problem. Frequent itemset mining techniques are able to efficiently capture the characteristics of (complex) data and succinctly summarize it. Owing to these and other interesting properties, these techniques have proven their value in biological data analysis. Nevertheless, information about the bioinfor-matics applications of these techniques remains scattered. In this primer, we introduce frequent itemset mining and their derived association rules for life scientists. We give an overview of various algorithms, and illustrate how they can be used in several real-life bioinformatics application domains. We end with a discussion of the future poten-tial and open challenges for frequent itemset mining in the life sciences.},
author = {Naulaerts, Stefan and Meysman, Pieter and Bittremieux, Wout and Vu, Trung Nghia and Berghe, Wim Vanden and Goethals, Bart and Laukens, Kris},
doi = {10.1093/bib/bbt074},
keywords = {association rule,biclustering,frequent item set,market basket analysis,pattern mining},
title = {{A primer to frequent itemset mining for bioinformatics}}
}
@misc{Information,
author = {NCBI, National Center for Biotechnology Information},
title = {{NCBI}},
url = {http://www.ncbi.nlm.nih.gov/probe/docs/techmicroarray/}
}
@misc{Information,
author = {NCBI, National Center for Biotechnology Information},
title = {{NCBI}},
url = {http://www.ncbi.nlm.nih.gov/probe/docs/techmicroarray/}
}
@article{Neath2006,
annote = {Aproximaci{\'{o}}n bayesiana al problema, leer y mirar cuidadosamente.

Hace uso de modelo normal homoced{\'{a}}stico.

Se plantea la idea de las particiones.

No s{\'{e}} c{\'{o}}mo obtienen los valores para seleccionar (me interesa?)},
author = {Neath, Andrew a and Cavanaugh, Joseph E},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Neath, Cavanaugh - 2006 - A Bayesian Approach to the Multiple Comparisons Problem.pdf:pdf},
journal = {Journal of Data Science},
keywords = {bayesian information criterion,hierarchical modelling,model},
pages = {131--146},
title = {{A Bayesian Approach to the Multiple Comparisons Problem}},
volume = {4},
year = {2006}
}
@article{CAMPOELIAS,
author = {Pardo, C E and {Del Campo}, P},
journal = {Revista Colombiana de Estad{\'{i}}stica},
pages = {231--245},
title = {{Combinaci{\'{o}}n de m{\'{e}}todos factoriales y de an{\'{a}}lisis de comglomerados en R}},
volume = {30},
year = {2007}
}
@article{CAMPOELIAS,
author = {Pardo, C E and {Del Campo}, P},
journal = {revista Colombiana de Estad{\{}{\'{i}}{\}}stica},
pages = {231--245},
title = {{Combinaci{\{}{\'{o}}{\}}n de m{\{}{\'{e}}{\}}todos factoriales y de an{\{}{\'{a}}{\}}lisis de comglomerados en R}},
volume = {30},
year = {2007}
}
@article{Parnell2014,
abstract = {BACKGROUND: Genetic understanding of complex traits has developed immensely over the past decade but remains hampered by incomplete descriptions of contribution to phenotypic variance. Gene-environment (GxE) interactions are one of these contributors and in the guise of diet and physical activity are important modulators of cardiometabolic phenotypes and ensuing diseases.$\backslash$n$\backslash$nRESULTS: We mined the scientific literature to collect GxE interactions from 386 publications for blood lipids, glycemic traits, obesity anthropometrics, vascular measures, inflammation and metabolic syndrome, and introduce CardioGxE, a gene-environment interaction resource. We then analyzed the genes and SNPs supporting cardiometabolic GxEs in order to demonstrate utility of GxE SNPs and to discern characteristics of these important genetic variants. We were able to draw many observations from our extensive analysis of GxEs. 1) The CardioGxE SNPs showed little overlap with variants identified by main effect GWAS, indicating the importance of environmental interactions with genetic factors on cardiometabolic traits. 2) These GxE SNPs were enriched in adaptation to climatic and geographical features, with implications on energy homeostasis and response to physical activity. 3) Comparison to gene networks responding to plasma cholesterol-lowering or regression of atherosclerotic plaques showed that GxE genes have a greater role in those responses, particularly through high-energy diets and fat intake, than do GWAS-identified genes for the same traits. Other aspects of the CardioGxE dataset were explored.$\backslash$n$\backslash$nCONCLUSIONS: Overall, we demonstrate that SNPs supporting cardiometabolic GxE interactions often exhibit transcriptional effects or are under positive selection. Still, not all such SNPs can be assigned potential functional or regulatory roles often because data are lacking in specific cell types or from treatments that approximate the environmental factor of the GxE. With research on metabolic related complex disease risk embarking on genome-wide GxE interaction tests, CardioGxE will be a useful resource.},
author = {Parnell, Laurence D and Blokker, Britt A and Dashti, Hassan S and Nesbeth, Paula-Dene and Cooper, Brittany Elle and Ma, Yiyi and Lee, Yu-Chi and Hou, Ruixue and Lai, Chao-Qiang and Richardson, Kris and Ordov{\'{a}}s, Jos{\'{e}} M},
doi = {10.1186/1756-0381-7-21},
issn = {1756-0381},
journal = {BioData mining},
pages = {21},
pmid = {25368670},
title = {{CardioGxE, a catalog of gene-environment interactions for cardiometabolic traits.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4217104{\{}{\&}{\}}tool=pmcentrez{\{}{\&}{\}}rendertype=abstract},
volume = {7},
year = {2014}
}
@article{PARZEN,
author = {Parzen, E},
journal = {Annals of Mathematical Statistics},
pages = {1065--1076},
title = {{On Estimation of a Probability Density Function and Mode}},
volume = {33},
year = {1962}
}
@article{PERRY,
author = {Perry, G and Miller, B and Enright, N},
journal = {Plant Ecology},
pages = {57--82},
title = {{A comparison of methods for the statistical analysis of spatial point patterns in plant ecology}},
volume = {187},
year = {2006}
}
@article{Pirooznia2014,
author = {Pirooznia, Mehdi and Kramer, Melissa and Parla, Jennifer and Goes, Fernando S and Potash, James B and McCombie, W and Zandi, Peter P},
doi = {10.1186/1479-7364-8-14},
issn = {1479-7364},
journal = {Human Genomics},
keywords = {Exome sequencing,Next-generation sequencing,Variant calling pipelines,exome sequencing,next-generation sequencing,variant calling pipelines},
number = {1},
pages = {14},
title = {{Validation and assessment of variant calling pipelines for next-generation sequencing}},
url = {http://www.humgenomics.com/content/8/1/14},
volume = {8},
year = {2014}
}
@article{Ramsay1991,
abstract = {Multivariate data analysis permits the study of observations which are finite sets of numbers, but modern data collection situations can involve data, or the processes giving rise to them, which are functions. Functinal data analysis involves infinite dimensional processes and/or data. The paper shows how the theory of L-splines can support generalizations of linear modelling and principal components analysis to samples drawn from random functions. Spline smoothing rests on a partition of a function space into two orthogonal subspaces, one of which contains the obvious or structural components of variation among a set of observed functions, and the other of which contains residual components. This partitioning is achieved through the use of a linear differential operator, and we show how the theory or polynomial splines can be applied more generally with an arbitrary operator and associated boundary constraints. These data analysis tools are illustrated by a study of variation in temperature-precipitation patterns among some Canadian weather-stations.},
author = {Ramsay, J O and Dalzell, C J},
journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
keywords = {Differential Operator,Functional Principal Component,Functional linear model,L-splines,Nonparametric Regression,Smoothing},
number = {3},
pages = {539--572},
title = {{Some Tools for Functional Data Analysis}},
url = {http://www.jstor.org/stable/2345586},
volume = {53},
year = {1991}
}
@article{Ramsay1991,
abstract = {Multivariate data analysis permits the study of observations which are finite sets of numbers, but modern data collection situations can involve data, or the processes giving rise to them, which are functions. Functinal data analysis involves infinite dimensional processes and/or data. The paper shows how the theory of L-splines can support generalizations of linear modelling and principal components analysis to samples drawn from random functions. Spline smoothing rests on a partition of a function space into two orthogonal subspaces, one of which contains the obvious or structural components of variation among a set of observed functions, and the other of which contains residual components. This partitioning is achieved through the use of a linear differential operator, and we show how the theory or polynomial splines can be applied more generally with an arbitrary operator and associated boundary constraints. These data analysis tools are illustrated by a study of variation in temperature-precipitation patterns among some Canadian weather-stations.},
author = {Ramsay, J.O. and Dalzell, C.J.},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ramsay, Dalzell - 1991 - Some Tools for Functional Data Analysis.pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
keywords = {Differential Operator,Functional Principal Component,Functional linear model,L-splines,Nonparametric Regression,Smoothing},
number = {3},
pages = {539--572},
title = {{Some Tools for Functional Data Analysis}},
url = {http://www.jstor.org/stable/2345586},
volume = {53},
year = {1991}
}
@misc{R,
address = {Vienna, Austria},
author = {{R Core Development Team}},
institution = {R Foundation for Statistical Computing},
isbn = {3-900051-07-0},
title = {{R: a language and environment for statistical computing, 3.1.2 ed.}},
url = {http://www.r-project.org/},
year = {2014}
}
@misc{RCoreTeam2015,
address = {Vienna, Austria},
author = {{R Core Team}},
institution = {R Foundation for Statistical Computing},
title = {{R: A Language and Environment for Statistical Computing}},
url = {https://www.r-project.org/},
year = {2015}
}
@article{Rencher,
author = {Rencher, Alvin C},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rencher - Unknown - Methods of Multivariate Analysis Second Edition.pdf:pdf},
title = {{Methods of Multivariate Analysis Second Edition}}
}
@book{RIPLEY,
address = {University of London},
author = {Ripley, B D},
publisher = {John Wiley {\&} Sons},
title = {{Spatial statistics}},
year = {1981}
}
@article{ROSENBLATT,
author = {Rosenblatt, M},
journal = {Annals of Mathematical Statistics},
pages = {832--837},
title = {{Remarks on some nonparametric estimates of a density function}},
volume = {27},
year = {1955}
}
@article{emd,
abstract = {W troduce a metric bet een t o distributions that w ein w w e call the Earth Movers Distanc e EMD The EMD is based on the minim al cost thatm ust be paid to transform one distribution in to the other in a precise sense W e sho w that the EMD has attractiv e properties for con ten tbased image retriev al The most importan t one as w e sho w is that it matc hes perceptual similarit y better than other distances used for image retriev al The EMD is based on a solution to the transportation problem from linear optimization for whic h ecien t algorithms are a ailable and also allo v ws naturally for partial matc hing It is more robust than histogram matc hing tec hniques in that it can operate on v ariablelength represen tations of the distributions that a oid v quan tization and other binning problems t ypical of histograms When used to compare distributions with the same o erall mass the EMD is a true metric In this paper w v e focus on applications to color and texture and w e compare the retriev al performance of the EMD with that of other distances},
author = {Rubner, Yossi and Tomasi, Carlo and Guibas, Leonidas J},
doi = {10.1023/A:1026543900054},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rubner, Tomasi, Guibas - 2000 - The Earth Mover ' s Distance as a Metric for Image Retrieval.pdf:pdf},
isbn = {0780393325},
issn = {0920-5691},
journal = {Work},
keywords = {color,earth mover,image retrieval,perceptual metrics,s distance,texture},
number = {2},
pages = {1--20},
title = {{The Earth Mover ' s Distance as a Metric for Image Retrieval}},
url = {http://www.springerlink.com/index/W5515K817681125H.pdf},
volume = {40},
year = {2000}
}
@article{emd,
abstract = {W troduce a metric bet een t o distributions that w ein w w e call the Earth Movers Distanc e EMD The EMD is based on the minim al cost thatm ust be paid to transform one distribution in to the other in a precise sense W e sho w that the EMD has attractiv e properties for con ten tbased image retriev al The most importan t one as w e sho w is that it matc hes perceptual similarit y better than other distances used for image retriev al The EMD is based on a solution to the transportation problem from linear optimization for whic h ecien t algorithms are a ailable and also allo v ws naturally for partial matc hing It is more robust than histogram matc hing tec hniques in that it can operate on v ariablelength represen tations of the distributions that a oid v quan tization and other binning problems t ypical of histograms When used to compare distributions with the same o erall mass the EMD is a true metric In this paper w v e focus on applications to color and texture and w e compare the retriev al performance of the EMD with that of other distances},
author = {Rubner, Yossi and Tomasi, Carlo and Guibas, Leonidas J},
doi = {10.1023/A:1026543900054},
isbn = {0780393325},
issn = {0920-5691},
journal = {Work},
keywords = {color,earth mover,image retrieval,perceptual metrics,s distance,texture},
number = {2},
pages = {1--20},
title = {{The Earth Mover ' s Distance as a Metric for Image Retrieval}},
url = {http://www.springerlink.com/index/W5515K817681125H.pdf},
volume = {40},
year = {2000}
}
@article{SAIN,
author = {Sain, S R and Baggerly, K A and Scott, D W},
journal = {Journal of the American Statistical Association},
pages = {807--817},
title = {{Cross-Validation of Multivariate Densities}},
volume = {89(427)},
year = {1994}
}
@article{SCOTT,
author = {Scott, D W and Terrell, G R},
journal = {Journal of the American Statistical Association},
pages = {1131--1146},
title = {{Biased and Unbiased Cross-Validation in Density Estimation}},
volume = {82(400)},
year = {1987}
}
@book{SHAO,
address = {Madison, WI 53706-1685},
author = {Shao, J},
publisher = {Springer},
title = {{Mathematical Statistics}},
year = {2003}
}
@article{SHAR,
author = {Sharma, A and Lall, U and Tarboton, D G},
journal = {Statistic Hydrology and Hydraulics},
pages = {33--52},
title = {{Kernel bandwidth selection for a first order nonparametric streamflow simulation model}},
volume = {12},
year = {1998}
}
@article{Shotton2013,
abstract = {We propose a new method to quickly and accurately predict 3D positions of body joints from a single depth image, using no temporal information. We take an object recognition approach, designing an intermediate body parts representation that maps the difficult pose estimation problem into a simpler per-pixel classification problem. Our large and highly varied training dataset allows the classifier to estimate body parts invariant to pose, body shape, clothing, etc. Finally we generate confidence-scored 3D proposals of several body joints by reprojecting the classification result and finding local modes. The system runs at 200 frames per second on consumer hardware. Our evaluation shows high accuracy on both synthetic and real test sets, and investigates the effect of several training parameters. We achieve state of the art accuracy in our comparison with related work and demonstrate improved generalization over exact whole-skeleton nearest neighbor matching.},
author = {Shotton, Jamie and Fitzgibbon, Andrew and Cook, Mat and Sharp, Toby and Finocchio, Mark and Moore, Richard and Kipman, Alex and Blake, Andrew},
journal = {Studies in Computational Intelligence},
pages = {119--135},
title = {{Real-time human pose recognition in parts from single depth images}},
volume = {411},
year = {2013}
}
@article{Showcase2004,
abstract = {CATS – Clustering After Transformation and Smoothing – is a technique for nonparametrically estimating and clustering a large number of curves. Our motivating example is a genetic microar-ray experiment but the method is very general. The method in-cludes: transformation and smoothing multiple curves, multiple nonparametric testing for trends, clustering curves with similar shape, and nonparametrically inferring the misclustering rate.},
author = {Showcase, Research and Cmu, @ and Serban, Nicoleta and Wasserman, Larry},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Showcase et al. - 2004 - CATS Clustering After Transformation and Smoothing.pdf:pdf},
keywords = {False discovery rate,Multiple testing,and phrases,clustering,genetic microarrays,misclustering,smoothing},
title = {{CATS: Clustering After Transformation and Smoothing}},
url = {http://repository.cmu.edu/statistics},
year = {2004}
}
@book{Soames2006,
author = {Soames, Scott and Misak, Cheryl},
isbn = {063119083X},
title = {{by Edited by}},
volume = {132},
year = {2006}
}
@book{Soames2006,
author = {Soames, Scott and Misak, Cheryl},
isbn = {063119083X},
title = {{by Edited by}},
volume = {132},
year = {2006}
}
@book{Staccini2014,
author = {Staccini, P and Daniel, C and Dart, T and Bouhaddou, O},
booktitle = {Springer},
doi = {10.1007/978-2-8178-0478-1},
isbn = {978-2-8178-0477-4},
pages = {315--348},
title = {{Medical Informatics, e-Health Fundamentals and Applications}},
url = {http://link.springer.com/10.1007/978-2-8178-0478-1{\{}{\_}{\}}13$\backslash$nhttp://link.springer.com/chapter/10.1007/978-2-8178-0478-1{\{}{\_}{\}}13},
year = {2014}
}
@article{Sun,
abstract = {This paper proposes an informative exploratory tool, the functional boxplot, for visualizing functional data, as well as its generalization, the enhanced functional boxplot. Based on the center outwards ordering induced by band depth for functional data, the descriptive statistics of a functional boxplot are: the envelope of the 50{\%} central region, the median curve and the maximum non-outlying envelope. In addition, outliers can be detected in a functional boxplot by the 1.5 times the 50{\%} central region empirical rule, analogous to the rule for classical boxplots. The construction of a functional boxplot is illustrated on a series of sea surface temperatures related to the El Ni˜ no phenomenon and its outlier detection performance is explored by simula- tions. As applications, the functional boxplot and enhanced functional boxplot are demonstrated on children growth data and spatio-temporal U.S. precipitation data for nine climatic regions, respectively.},
author = {Sun, Ying and Genton, Marc G},
doi = {10.1198/jcgs.2011.09224},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sun, Genton - 2010 - Functional Boxplots.pdf:pdf},
isbn = {1061-8600},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {Depth,Functional data,Growth data,Precipitation,data,depth,functional data,growth data,precipitation data,space,time,visualization},
number = {2},
pages = {1--19},
title = {{Functional Boxplots}},
volume = {20},
year = {2010}
}
@article{Systems2009,
author = {Systems, Computational and Group, Biology},
title = {{FASTQ format and data quality}},
year = {2009}
}
@article{Systems2009a,
author = {Systems, Computational and Group, Biology},
title = {{Structural variation and Medical Genomics}},
year = {2009}
}
@article{GCTHEO,
annote = {From Duplicate 1 (A Generalization of the Glivenko Cantelli Theorem - Tucker, H G)

From Duplicate 2 (A Generalization of the Glivenko Cantelli Theorem - Tucker, H G)

El teorema de glivenko Cantelli.

Mirar si es multivariado.},
author = {Tucker, Howard G.},
doi = {10.1214/193940307000000455},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tucker - 1959 - A Generalization of the Glivenko Cantelli Theorem.pdf:pdf;:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tucker - Unknown - A Generalization of the Glivenko-Cantelli Theorem.pdf:pdf},
isbn = {978-0-940600-74-4},
journal = {The Annals of Mathematical Statistic},
number = {3},
pages = {828--830},
title = {{A Generalization of the Glivenko-Cantelli Theorem}},
url = {http://projecteuclid.org/euclid.imsc/1207580085},
volume = {30},
year = {1959}
}
@misc{Tucker,
author = {Tucker, Howard G.},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tucker - Unknown - A Generalization of the Glivenko-Cantelli Theorem.pdf:pdf},
title = {{A Generalization of the Glivenko-Cantelli Theorem}}
}
@article{Ward1963,
abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. This content downloaded from 168.176.5.118 on Tue, 08 Dec 2015 16:53:29 UTC All use subject to JSTOR Terms and Conditions},
author = {Ward, Joe H},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ward - 1963 - Hierarchical Grouping to Optimize an Objective Function.pdf:pdf},
journal = {Source Journal of the American Statistical Association},
number = {301},
pages = {236--244},
title = {{Hierarchical Grouping to Optimize an Objective Function}},
url = {http://www.jstor.org http://www.jstor.org/stable/2282967 http://www.jstor.org/page/},
volume = {58},
year = {1963}
}
@article{WARD,
author = {{Ward Jr}, J},
journal = {Journal of the American Statistical Association},
pages = {236--244},
title = {{Hierarchical grouping to optimize an objective function}},
volume = {58},
year = {1963}
}
@article{WATSON,
author = {Watson, G},
journal = {Sankhya: The Indian Journal of Statistics, Series A},
pages = {372--379},
title = {{Smooth regression analysis.}},
volume = {26},
year = {1964}
}
@article{Weitschek2014,
author = {Weitschek, Emanuel and Fiscon, Giulia and Felici, Giovanni},
doi = {10.1186/1756-0381-7-4},
issn = {1756-0381},
journal = {BioData Mining},
keywords = {DNA Barcoding,Species identification,Supervised classification methods},
number = {1},
pages = {4},
publisher = {BioData Mining},
title = {{Supervised DNA Barcodes species classification: analysis, comparisons and results}},
url = {http://www.biodatamining.org/content/7/1/4},
volume = {7},
year = {2014}
}
@article{Wilcoxon1946,
abstract = {The objective of the present paper is to indicate the possibility of using ranking methods.},
author = {Wilcoxon, F},
doi = {10.2307/3001968},
isbn = {0099-4987},
issn = {00994987},
journal = {Journal of economic entomology},
keywords = {STATISTICS},
number = {6},
pages = {269},
pmid = {20983181},
title = {{Individual comparisons of grouped data by ranking methods.}},
volume = {39},
year = {1946}
}
@article{Wilcoxon1946,
abstract = {The objective of the present paper is to indicate the possibility of using ranking methods.},
author = {Wilcoxon, F},
doi = {10.2307/3001968},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wilcoxon - 1946 - Individual comparisons of grouped data by ranking methods.pdf:pdf},
isbn = {0099-4987},
issn = {00994987},
journal = {Journal of economic entomology},
keywords = {STATISTICS},
number = {6},
pages = {269},
pmid = {20983181},
title = {{Individual comparisons of grouped data by ranking methods.}},
volume = {39},
year = {1946}
}
@article{Zaki2007,
abstract = {This is a meeting report for the 6th SIGKDD Workshop on Data Mining in Bioinformatics.},
author = {Zaki, Mohammed J and Karypis, George and Yang, Jiong},
doi = {10.1186/1748-7188-2-4},
isbn = {1852335831},
issn = {17487188},
journal = {Algorithms for molecular biology : AMB},
pages = {4},
pmid = {17428327},
title = {{Data Mining in Bioinformatics (BIOKDD).}},
volume = {2},
year = {2007}
}
@article{Zhao2014,
abstract = {To demonstrate the benefits of RNA-Seq over microarray in transcriptome profiling, both RNA-Seq and microarray analyses were performed on RNA samples from a human T cell activation experiment. In contrast to other reports, our analyses focused on the difference, rather than similarity, between RNA-Seq and microarray technologies in transcriptome profiling. A comparison of data sets derived from RNA-Seq and Affymetrix platforms using the same set of samples showed a high correlation between gene expression profiles generated by the two platforms. However, it also demonstrated that RNA-Seq was superior in detecting low abundance transcripts, differentiating biologically critical isoforms, and allowing the identification of genetic variants. RNA-Seq also demonstrated a broader dynamic range than microarray, which allowed for the detection of more differentially expressed genes with higher fold-change. Analysis of the two datasets also showed the benefit derived from avoidance of technical issues inherent to microarray probe performance such as cross-hybridization, non-specific hybridization and limited detection range of individual probes. Because RNA-Seq does not rely on a pre-designed complement sequence detection probe, it is devoid of issues associated with probe redundancy and annotation, which simplified interpretation of the data. Despite the superior benefits of RNA-Seq, microarrays are still the more common choice of researchers when conducting transcriptional profiling experiments. This is likely because RNA-Seq sequencing technology is new to most researchers, more expensive than microarray, data storage is more challenging and analysis is more complex. We expect that once these barriers are overcome, the RNA-Seq platform will become the predominant tool for transcriptome analysis.},
author = {Zhao, Shanrong and Fung-Leung, Wai-Ping and Bittner, Anton and Ngo, Karen and Liu, Xuejun},
doi = {10.1371/journal.pone.0078644},
isbn = {1932-6203},
issn = {1932-6203},
journal = {PloS one},
keywords = {Analysis of Variance,Base Sequence,CD4-Positive T-Lymphocytes,CD4-Positive T-Lymphocytes: metabolism,Cells,Cultured,Gene Expression Profiling,Humans,Lymphocyte Activation,Molecular Sequence Annotation,Oligonucleotide Array Sequence Analysis,Protein Isoforms,Protein Isoforms: genetics,Protein Isoforms: metabolism,RNA,Sequence Analysis,Transcriptome},
number = {1},
pages = {e78644},
pmid = {24454679},
title = {{Comparison of RNA-Seq and microarray in transcriptome profiling of activated T cells.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3894192{\{}{\{}{\}}{\{}{\&}{\}}{\{}{\}}{\}}tool=pmcentrez{\{}{\{}{\}}{\{}{\&}{\}}{\{}{\}}{\}}rendertype=abstract},
volume = {9},
year = {2014}
}
@article{Zhao2014,
abstract = {To demonstrate the benefits of RNA-Seq over microarray in transcriptome profiling, both RNA-Seq and microarray analyses were performed on RNA samples from a human T cell activation experiment. In contrast to other reports, our analyses focused on the difference, rather than similarity, between RNA-Seq and microarray technologies in transcriptome profiling. A comparison of data sets derived from RNA-Seq and Affymetrix platforms using the same set of samples showed a high correlation between gene expression profiles generated by the two platforms. However, it also demonstrated that RNA-Seq was superior in detecting low abundance transcripts, differentiating biologically critical isoforms, and allowing the identification of genetic variants. RNA-Seq also demonstrated a broader dynamic range than microarray, which allowed for the detection of more differentially expressed genes with higher fold-change. Analysis of the two datasets also showed the benefit derived from avoidance of technical issues inherent to microarray probe performance such as cross-hybridization, non-specific hybridization and limited detection range of individual probes. Because RNA-Seq does not rely on a pre-designed complement sequence detection probe, it is devoid of issues associated with probe redundancy and annotation, which simplified interpretation of the data. Despite the superior benefits of RNA-Seq, microarrays are still the more common choice of researchers when conducting transcriptional profiling experiments. This is likely because RNA-Seq sequencing technology is new to most researchers, more expensive than microarray, data storage is more challenging and analysis is more complex. We expect that once these barriers are overcome, the RNA-Seq platform will become the predominant tool for transcriptome analysis.},
author = {Zhao, Shanrong and Fung-Leung, Wai-Ping and Bittner, Anton and Ngo, Karen and Liu, Xuejun},
doi = {10.1371/journal.pone.0078644},
file = {:home/julian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao et al. - 2014 - Comparison of RNA-Seq and microarray in transcriptome profiling of activated T cells.pdf:pdf},
isbn = {1932-6203},
issn = {1932-6203},
journal = {PloS one},
keywords = {Analysis of Variance,Base Sequence,CD4-Positive T-Lymphocytes,CD4-Positive T-Lymphocytes: metabolism,Cells,Cultured,Gene Expression Profiling,Humans,Lymphocyte Activation,Molecular Sequence Annotation,Oligonucleotide Array Sequence Analysis,Protein Isoforms,Protein Isoforms: genetics,Protein Isoforms: metabolism,RNA,Sequence Analysis,Transcriptome},
number = {1},
pages = {e78644},
pmid = {24454679},
title = {{Comparison of RNA-Seq and microarray in transcriptome profiling of activated T cells.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3894192{\{}{\&}{\}}tool=pmcentrez{\{}{\&}{\}}rendertype=abstract},
volume = {9},
year = {2014}
}
